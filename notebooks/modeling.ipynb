{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e89be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.utils import load_processed_data\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9f3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "kick_clean = load_processed_data(use_relative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad84ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_final_irrelevant_columns_and_encode_target(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # These are useful for lookup / human insight, but not for machine learning\n",
    "    df = df.copy()\n",
    "    irrelevant_columns = [\n",
    "        \"index\",\n",
    "        \"name\",\n",
    "        \"blurb\",\n",
    "    ]\n",
    "    df = df.drop(columns=irrelevant_columns)\n",
    "\n",
    "    # Encode target\n",
    "    df[\"is_successful\"] = df[\"state\"] == \"successful\"\n",
    "    df = df.drop(columns=[\"state\"])\n",
    "    return df\n",
    "\n",
    "def handle_datetime_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    datetime_columns = [\n",
    "        \"created_at\",\n",
    "        \"deadline\",\n",
    "        \"launched_at\"\n",
    "    ]\n",
    "\n",
    "    df[\"age_days\"] = (pd.Timestamp.now() - df[\"launched_at\"]).dt.days\n",
    "\n",
    "    df = df.drop(columns = datetime_columns)\n",
    "    return df\n",
    "\n",
    "def machine_ready_preprocessing(\n",
    "    df: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, LabelEncoder, StandardScaler]:\n",
    "    df = handle_datetime_features(df)\n",
    "    df = drop_final_irrelevant_columns_and_encode_target(df)\n",
    "    return df\n",
    "\n",
    "kick_transformed = machine_ready_preprocessing(kick_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "#               CONFIG\n",
    "# ====================================\n",
    "K_splits = 5\n",
    "random_state = 1337\n",
    "\n",
    "def preprocess_data(X_train, X_test, categorical_cols, numerical_cols):\n",
    "    # Training set will fit the encoders / scalers, the test set will use the fitted encoders / scalers\n",
    "    # ====================================\n",
    "    #        CATEGORICAL ENCODING\n",
    "    # ====================================\n",
    "    # Encode categorical features\n",
    "    # OneHotEncoding for features that have relatively low cardinality (< 100 unique values)\n",
    "    # FrequencyEncoding for features that have relatively high cardinality (>= 100 unique values)\n",
    "    for cat_col in categorical_cols:\n",
    "        if X_train[cat_col].nunique() < 100: # OneHotEncoding for low cardinality\n",
    "            oh_encoder = OneHotEncoder(\n",
    "                handle_unknown='ignore',\n",
    "                sparse_output=False\n",
    "            )\n",
    "            oh_encoder.fit(X_train[[cat_col]])\n",
    "\n",
    "            # Transform and join back to dataframe\n",
    "            X_train_oh = pd.DataFrame(\n",
    "                oh_encoder.transform(X_train[[cat_col]]),\n",
    "                columns=[f\"{cat_col}__{cat}\" for cat in oh_encoder.categories_[0]],\n",
    "                index=X_train.index\n",
    "            )\n",
    "            X_test_oh = pd.DataFrame(\n",
    "                oh_encoder.transform(X_test[[cat_col]]),\n",
    "                columns=[f\"{cat_col}__{cat}\" for cat in oh_encoder.categories_[0]],\n",
    "                index=X_test.index\n",
    "            )\n",
    "\n",
    "            X_train = X_train.drop(columns=[cat_col]).join(X_train_oh)\n",
    "            X_test = X_test.drop(columns=[cat_col]).join(X_test_oh)\n",
    "        else: # FrequencyEncoding for high cardinality\n",
    "            freq_encoding = X_train[cat_col].value_counts(normalize=True)\n",
    "            X_train.loc[:, cat_col] = X_train[cat_col].map(freq_encoding)\n",
    "            X_test.loc[:, cat_col] = X_test[cat_col].map(freq_encoding).fillna(1 / (X_train.__len__() + X_test.__len__())) # Fill unseen with \"rare\" frequency\n",
    "            X_train[cat_col] = X_train[cat_col].astype(\"float64\")\n",
    "            X_test[cat_col] = X_test[cat_col].astype(\"float64\")\n",
    "    \n",
    "    # ====================================\n",
    "    #         NUMERICAL SCALING\n",
    "    # ====================================\n",
    "\n",
    "    num_scaler = StandardScaler()\n",
    "    num_scaler.fit(X_train[numerical_cols])\n",
    "    X_train.loc[:, numerical_cols] = num_scaler.transform(X_train[numerical_cols])\n",
    "    X_test.loc[:, numerical_cols] = num_scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def perform_CV(\n",
    "        data: pd.DataFrame, \n",
    "        model_class, \n",
    "        model_params, \n",
    "        verbose = False\n",
    "    ) -> list[dict[str, float]]:\n",
    "    df = data.copy()\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    numerical_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    train = df.drop(columns=[\"is_successful\"])\n",
    "    target = df[\"is_successful\"]\n",
    "\n",
    "    # Ensure numerical columns are float64\n",
    "    train[numerical_cols] = train[numerical_cols].astype(\"float64\")\n",
    "\n",
    "    # ====================================\n",
    "    #        CROSS-VALIDATION LOOP\n",
    "    # ====================================\n",
    "    fold_scores = []\n",
    "    sk_fold = StratifiedKFold(n_splits=K_splits, shuffle=True, random_state=random_state)\n",
    "    for i, (train_idx, test_idx) in enumerate(sk_fold.split(train, target)):\n",
    "        if verbose:\n",
    "            print(f\"Fold {i+1}\")\n",
    "\n",
    "        X_train, X_test = train.iloc[train_idx], train.iloc[test_idx]\n",
    "        y_train, y_test = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "        # APPLY PREPROCESSING\n",
    "        X_train, X_test = preprocess_data(X_train, X_test, categorical_cols, numerical_cols)\n",
    "\n",
    "        # ====================================\n",
    "        #         TRAIN AND EVALUTATE\n",
    "        # ====================================\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        fold_result = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        }\n",
    "        fold_scores.append(fold_result)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\taccuracy={fold_result['accuracy']:.4f}, \\n\"\n",
    "                f\"\\tprecision={fold_result['precision']:.4f}, \\n\"\n",
    "                f\"\\trecall={fold_result['recall']:.4f}, \\n\"\n",
    "                f\"\\tf1={fold_result['f1']:.4f}\"\n",
    "            )\n",
    "    return fold_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27983bac",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 13:06:49,960] A new study created in memory with name: rf_f1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1b8a79761c495a85154a0ff47465a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 13:07:37,543] Trial 0 finished with value: 0.8561253620589275 and parameters: {'n_estimators': 73, 'max_depth': 16, 'min_samples_split': 19}. Best is trial 0 with value: 0.8561253620589275.\n",
      "[I 2025-11-15 13:08:39,559] Trial 1 finished with value: 0.8561951365244977 and parameters: {'n_estimators': 96, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 1 with value: 0.8561951365244977.\n",
      "[I 2025-11-15 13:08:49,278] Trial 2 finished with value: 0.7929651720133478 and parameters: {'n_estimators': 42, 'max_depth': 3, 'min_samples_split': 13}. Best is trial 1 with value: 0.8561951365244977.\n",
      "[I 2025-11-15 13:09:58,568] Trial 3 finished with value: 0.8639876440169967 and parameters: {'n_estimators': 73, 'max_depth': 30, 'min_samples_split': 5}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:10:19,882] Trial 4 finished with value: 0.8537609609619728 and parameters: {'n_estimators': 29, 'max_depth': 16, 'min_samples_split': 14}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:11:04,568] Trial 5 finished with value: 0.8617206193181317 and parameters: {'n_estimators': 52, 'max_depth': 24, 'min_samples_split': 16}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:11:15,443] Trial 6 finished with value: 0.8501127002128015 and parameters: {'n_estimators': 11, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:11:50,629] Trial 7 finished with value: 0.8424634458747648 and parameters: {'n_estimators': 89, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:12:04,571] Trial 8 finished with value: 0.8537428031914056 and parameters: {'n_estimators': 16, 'max_depth': 17, 'min_samples_split': 10}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:12:56,231] Trial 9 finished with value: 0.8598765061959106 and parameters: {'n_estimators': 67, 'max_depth': 20, 'min_samples_split': 14}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:14:07,364] Trial 10 finished with value: 0.8641887512385175 and parameters: {'n_estimators': 76, 'max_depth': 30, 'min_samples_split': 6}. Best is trial 10 with value: 0.8641887512385175.\n",
      "[I 2025-11-15 13:15:17,143] Trial 11 finished with value: 0.8644150198031266 and parameters: {'n_estimators': 75, 'max_depth': 29, 'min_samples_split': 6}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:16:35,473] Trial 12 finished with value: 0.8640950104144276 and parameters: {'n_estimators': 86, 'max_depth': 28, 'min_samples_split': 7}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:17:26,814] Trial 13 finished with value: 0.862527581902742 and parameters: {'n_estimators': 57, 'max_depth': 25, 'min_samples_split': 7}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:18:40,565] Trial 14 finished with value: 0.8636111513687407 and parameters: {'n_estimators': 81, 'max_depth': 25, 'min_samples_split': 2}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:19:35,273] Trial 15 finished with value: 0.8631469772877052 and parameters: {'n_estimators': 58, 'max_depth': 30, 'min_samples_split': 7}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:20:57,522] Trial 16 finished with value: 0.8628021463132216 and parameters: {'n_estimators': 99, 'max_depth': 22, 'min_samples_split': 4}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:21:19,203] Trial 17 finished with value: 0.8459429057146336 and parameters: {'n_estimators': 42, 'max_depth': 11, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:22:21,553] Trial 18 finished with value: 0.8639616011212755 and parameters: {'n_estimators': 69, 'max_depth': 27, 'min_samples_split': 8}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:23:26,706] Trial 19 finished with value: 0.86101066232453 and parameters: {'n_estimators': 83, 'max_depth': 21, 'min_samples_split': 12}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:24:11,098] Trial 20 finished with value: 0.861887902711221 and parameters: {'n_estimators': 47, 'max_depth': 27, 'min_samples_split': 4}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:25:28,590] Trial 21 finished with value: 0.8643029084862388 and parameters: {'n_estimators': 84, 'max_depth': 30, 'min_samples_split': 6}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:26:39,454] Trial 22 finished with value: 0.8641108196160339 and parameters: {'n_estimators': 77, 'max_depth': 29, 'min_samples_split': 6}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:27:39,440] Trial 23 finished with value: 0.8631800648135679 and parameters: {'n_estimators': 65, 'max_depth': 26, 'min_samples_split': 3}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:29:00,945] Trial 24 finished with value: 0.864321936700031 and parameters: {'n_estimators': 90, 'max_depth': 30, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:30:17,668] Trial 25 finished with value: 0.8632067104254778 and parameters: {'n_estimators': 93, 'max_depth': 23, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:31:37,323] Trial 26 finished with value: 0.8642582423059775 and parameters: {'n_estimators': 90, 'max_depth': 28, 'min_samples_split': 11}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:32:39,781] Trial 27 finished with value: 0.8606793156508676 and parameters: {'n_estimators': 82, 'max_depth': 20, 'min_samples_split': 8}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:32:57,417] Trial 28 finished with value: 0.7927264733532887 and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:33:25,776] Trial 29 finished with value: 0.8439892209268823 and parameters: {'n_estimators': 64, 'max_depth': 10, 'min_samples_split': 17}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:58:11,326] Trial 30 finished with value: 0.8625552199351487 and parameters: {'n_estimators': 78, 'max_depth': 26, 'min_samples_split': 19}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:59:32,571] Trial 31 finished with value: 0.8640123869154799 and parameters: {'n_estimators': 89, 'max_depth': 28, 'min_samples_split': 11}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:00:55,289] Trial 32 finished with value: 0.8637176261684294 and parameters: {'n_estimators': 93, 'max_depth': 28, 'min_samples_split': 11}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:02:20,544] Trial 33 finished with value: 0.8643495286943541 and parameters: {'n_estimators': 93, 'max_depth': 30, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:03:46,902] Trial 34 finished with value: 0.8643413369219319 and parameters: {'n_estimators': 95, 'max_depth': 30, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:04:53,927] Trial 35 finished with value: 0.8643310031240041 and parameters: {'n_estimators': 72, 'max_depth': 30, 'min_samples_split': 8}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:05:56,495] Trial 36 finished with value: 0.8636627949991468 and parameters: {'n_estimators': 73, 'max_depth': 25, 'min_samples_split': 13}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:06:47,673] Trial 37 finished with value: 0.8509424757574218 and parameters: {'n_estimators': 96, 'max_depth': 13, 'min_samples_split': 8}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:07:40,679] Trial 38 finished with value: 0.8622236487010178 and parameters: {'n_estimators': 62, 'max_depth': 24, 'min_samples_split': 10}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:08:45,304] Trial 39 finished with value: 0.8631195505752366 and parameters: {'n_estimators': 71, 'max_depth': 27, 'min_samples_split': 5}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:09:03,286] Trial 40 finished with value: 0.8352527417407991 and parameters: {'n_estimators': 51, 'max_depth': 7, 'min_samples_split': 12}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:10:27,546] Trial 41 finished with value: 0.8642536432679677 and parameters: {'n_estimators': 94, 'max_depth': 30, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:11:44,962] Trial 42 finished with value: 0.8642351580819267 and parameters: {'n_estimators': 86, 'max_depth': 29, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:13:05,402] Trial 43 finished with value: 0.8646448835621587 and parameters: {'n_estimators': 89, 'max_depth': 29, 'min_samples_split': 8}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 14:14:14,149] Trial 44 finished with value: 0.864357024386979 and parameters: {'n_estimators': 75, 'max_depth': 29, 'min_samples_split': 7}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 14:14:39,595] Trial 45 finished with value: 0.8596903792862653 and parameters: {'n_estimators': 26, 'max_depth': 26, 'min_samples_split': 7}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 14:15:47,359] Trial 46 finished with value: 0.8631953224233808 and parameters: {'n_estimators': 80, 'max_depth': 23, 'min_samples_split': 5}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 15:48:32,399] Trial 47 finished with value: 0.864159981775001 and parameters: {'n_estimators': 99, 'max_depth': 28, 'min_samples_split': 10}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 15:49:52,594] Trial 48 finished with value: 0.8641860766582352 and parameters: {'n_estimators': 87, 'max_depth': 29, 'min_samples_split': 6}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 15:51:16,345] Trial 49 finished with value: 0.8632952688974985 and parameters: {'n_estimators': 97, 'max_depth': 24, 'min_samples_split': 7}. Best is trial 43 with value: 0.8646448835621587.\n",
      "RF best F1: 0.8646448835621587\n",
      "RF best params: {'n_estimators': 89, 'max_depth': 29, 'min_samples_split': 8}\n"
     ]
    }
   ],
   "source": [
    "def objective_rf(trial: optuna.Trial) -> float:\n",
    "    model_params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        # \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "    }\n",
    "\n",
    "    fold_scores = perform_CV(\n",
    "        data=kick_transformed,\n",
    "        model_class=RandomForestClassifier,\n",
    "        model_params=model_params,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "    return mean_f1\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_rf = optuna.create_study(direction=\"maximize\", study_name=\"rf_f1\")\n",
    "study_rf.optimize(objective_rf, n_trials=50)\n",
    "\n",
    "print(\"RF best F1:\", study_rf.best_value)\n",
    "print(\"RF best params:\", study_rf.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87a4ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 16:20:52,080] A new study created in memory with name: xgb_accuracy\n",
      "[I 2025-11-16 16:21:00,263] Trial 0 finished with value: 0.8369615488034418 and parameters: {'n_estimators': 235, 'max_depth': 11, 'learning_rate': 0.164676505441507, 'gamma': 1.2534856197539428}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:21:06,920] Trial 1 finished with value: 0.7933476741059424 and parameters: {'n_estimators': 236, 'max_depth': 4, 'learning_rate': 0.017384966060298823, 'gamma': 1.340703053242787}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:21:12,144] Trial 2 finished with value: 0.7674374831944071 and parameters: {'n_estimators': 115, 'max_depth': 2, 'learning_rate': 0.03158451498850207, 'gamma': 1.6854772281795716}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:21:18,124] Trial 3 finished with value: 0.6207098682441516 and parameters: {'n_estimators': 193, 'max_depth': 2, 'learning_rate': 0.001976510057580965, 'gamma': 0.43086526140227843}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:21:27,934] Trial 4 finished with value: 0.8194353320785158 and parameters: {'n_estimators': 220, 'max_depth': 8, 'learning_rate': 0.018627203285256546, 'gamma': 2.1496920102865875}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:21:45,208] Trial 5 finished with value: 0.7224468943264319 and parameters: {'n_estimators': 307, 'max_depth': 11, 'learning_rate': 0.0010489775992805692, 'gamma': 3.398758267926631}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:21:57,887] Trial 6 finished with value: 0.7885937079860178 and parameters: {'n_estimators': 207, 'max_depth': 10, 'learning_rate': 0.0035837940267022007, 'gamma': 1.8743470757122271}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:22:20,064] Trial 7 finished with value: 0.7822909384243076 and parameters: {'n_estimators': 247, 'max_depth': 12, 'learning_rate': 0.002112182331678432, 'gamma': 0.3128770292807581}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:22:29,660] Trial 8 finished with value: 0.8188168862597471 and parameters: {'n_estimators': 242, 'max_depth': 8, 'learning_rate': 0.016050970973100962, 'gamma': 3.757420669349628}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:22:37,561] Trial 9 finished with value: 0.8241032535627857 and parameters: {'n_estimators': 319, 'max_depth': 5, 'learning_rate': 0.036652389538520236, 'gamma': 3.615707666530051}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:22:44,409] Trial 10 finished with value: 0.8349287442860984 and parameters: {'n_estimators': 389, 'max_depth': 10, 'learning_rate': 0.23319768541607566, 'gamma': 4.5480267799892005}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:22:51,787] Trial 11 finished with value: 0.8346598547996773 and parameters: {'n_estimators': 385, 'max_depth': 10, 'learning_rate': 0.2224247678605879, 'gamma': 4.671089409942778}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:22:57,644] Trial 12 finished with value: 0.8348104329120731 and parameters: {'n_estimators': 52, 'max_depth': 9, 'learning_rate': 0.2407992804482393, 'gamma': 4.701402162088553}. Best is trial 0 with value: 0.8369615488034418.\n",
      "[I 2025-11-16 16:23:09,280] Trial 13 finished with value: 0.8377198171551493 and parameters: {'n_estimators': 391, 'max_depth': 12, 'learning_rate': 0.0903421490426916, 'gamma': 2.748965065943108}. Best is trial 13 with value: 0.8377198171551493.\n",
      "[I 2025-11-16 16:23:19,889] Trial 14 finished with value: 0.8372411938693197 and parameters: {'n_estimators': 316, 'max_depth': 12, 'learning_rate': 0.0771829401175675, 'gamma': 2.768518908114686}. Best is trial 13 with value: 0.8377198171551493.\n",
      "[I 2025-11-16 16:23:29,966] Trial 15 finished with value: 0.837768217262705 and parameters: {'n_estimators': 329, 'max_depth': 12, 'learning_rate': 0.08052872877549497, 'gamma': 2.787236281659477}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:23:38,162] Trial 16 finished with value: 0.8356278569507932 and parameters: {'n_estimators': 355, 'max_depth': 6, 'learning_rate': 0.09236628939276494, 'gamma': 2.73724821041652}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:23:47,809] Trial 17 finished with value: 0.8370906157569239 and parameters: {'n_estimators': 349, 'max_depth': 12, 'learning_rate': 0.07601052566944545, 'gamma': 3.1631190143472274}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:23:57,802] Trial 18 finished with value: 0.7987415972035494 and parameters: {'n_estimators': 291, 'max_depth': 7, 'learning_rate': 0.006997451121002892, 'gamma': 4.089511515739309}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:24:06,426] Trial 19 finished with value: 0.837424038720086 and parameters: {'n_estimators': 395, 'max_depth': 9, 'learning_rate': 0.11932796456058892, 'gamma': 2.412023019488295}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:24:17,935] Trial 20 finished with value: 0.8376821726270502 and parameters: {'n_estimators': 277, 'max_depth': 11, 'learning_rate': 0.04432607558953399, 'gamma': 3.063347019659862}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:24:30,000] Trial 21 finished with value: 0.837321860715246 and parameters: {'n_estimators': 275, 'max_depth': 11, 'learning_rate': 0.04157004799527858, 'gamma': 3.0030695546056587}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:24:42,715] Trial 22 finished with value: 0.8375638612530251 and parameters: {'n_estimators': 349, 'max_depth': 12, 'learning_rate': 0.05044419538029223, 'gamma': 2.3191174778021324}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:24:51,313] Trial 23 finished with value: 0.836875504167787 and parameters: {'n_estimators': 169, 'max_depth': 11, 'learning_rate': 0.058182077689557875, 'gamma': 4.103807759005237}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:24:59,065] Trial 24 finished with value: 0.8362463027695617 and parameters: {'n_estimators': 359, 'max_depth': 9, 'learning_rate': 0.13335568295449166, 'gamma': 3.0580538759617175}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:25:15,151] Trial 25 finished with value: 0.8373433718741596 and parameters: {'n_estimators': 284, 'max_depth': 12, 'learning_rate': 0.025376520126007863, 'gamma': 2.5229853164304026}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:25:31,544] Trial 26 finished with value: 0.8265447700994892 and parameters: {'n_estimators': 330, 'max_depth': 10, 'learning_rate': 0.011024789636138758, 'gamma': 3.89307400510698}. Best is trial 15 with value: 0.837768217262705.\n",
      "[I 2025-11-16 16:25:42,178] Trial 27 finished with value: 0.837945684323743 and parameters: {'n_estimators': 370, 'max_depth': 11, 'learning_rate': 0.0676201390702402, 'gamma': 2.032817681899908}. Best is trial 27 with value: 0.837945684323743.\n",
      "[I 2025-11-16 16:25:52,523] Trial 28 finished with value: 0.8391234202742673 and parameters: {'n_estimators': 372, 'max_depth': 8, 'learning_rate': 0.11892109941269255, 'gamma': 0.8132744729685981}. Best is trial 28 with value: 0.8391234202742673.\n",
      "[I 2025-11-16 16:26:02,008] Trial 29 finished with value: 0.8388921753159451 and parameters: {'n_estimators': 369, 'max_depth': 7, 'learning_rate': 0.14201542147039414, 'gamma': 0.8397166221943158}. Best is trial 28 with value: 0.8391234202742673.\n",
      "[I 2025-11-16 16:26:10,895] Trial 30 finished with value: 0.8392363538585641 and parameters: {'n_estimators': 371, 'max_depth': 7, 'learning_rate': 0.1789693608288411, 'gamma': 0.766163101908516}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:26:20,236] Trial 31 finished with value: 0.8385103522452273 and parameters: {'n_estimators': 371, 'max_depth': 7, 'learning_rate': 0.13498844695673343, 'gamma': 0.7456717286223431}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:26:29,018] Trial 32 finished with value: 0.8385910190911534 and parameters: {'n_estimators': 369, 'max_depth': 7, 'learning_rate': 0.16262907936569207, 'gamma': 0.7696550574022508}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:26:37,340] Trial 33 finished with value: 0.8377789728421619 and parameters: {'n_estimators': 340, 'max_depth': 6, 'learning_rate': 0.17231135761747077, 'gamma': 1.0526209122016041}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:26:45,376] Trial 34 finished with value: 0.8373111051357892 and parameters: {'n_estimators': 372, 'max_depth': 6, 'learning_rate': 0.27810125184442874, 'gamma': 1.3720020187139808}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:26:53,625] Trial 35 finished with value: 0.8388437752083894 and parameters: {'n_estimators': 148, 'max_depth': 7, 'learning_rate': 0.18029745323690102, 'gamma': 0.016294852075202204}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:27:02,017] Trial 36 finished with value: 0.8378327507394461 and parameters: {'n_estimators': 124, 'max_depth': 8, 'learning_rate': 0.18085867423645374, 'gamma': 0.025882513081380343}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:27:09,135] Trial 37 finished with value: 0.8324818499596665 and parameters: {'n_estimators': 160, 'max_depth': 5, 'learning_rate': 0.12656671819376883, 'gamma': 1.577343282044144}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:27:14,623] Trial 38 finished with value: 0.8179241731648293 and parameters: {'n_estimators': 74, 'max_depth': 3, 'learning_rate': 0.29488383310395383, 'gamma': 0.5280165098430916}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:27:22,317] Trial 39 finished with value: 0.8353482118849153 and parameters: {'n_estimators': 111, 'max_depth': 8, 'learning_rate': 0.1061256772823454, 'gamma': 1.0218178046591588}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:27:31,889] Trial 40 finished with value: 0.8120462489916644 and parameters: {'n_estimators': 254, 'max_depth': 5, 'learning_rate': 0.0260387388381609, 'gamma': 0.23043725350353517}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:27:39,981] Trial 41 finished with value: 0.8389190642645872 and parameters: {'n_estimators': 164, 'max_depth': 7, 'learning_rate': 0.17592843116363574, 'gamma': 0.7049098824905013}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:27:47,866] Trial 42 finished with value: 0.8391879537510082 and parameters: {'n_estimators': 184, 'max_depth': 7, 'learning_rate': 0.17820032440427838, 'gamma': 0.6302482785944739}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:27:55,643] Trial 43 finished with value: 0.838827641839204 and parameters: {'n_estimators': 186, 'max_depth': 6, 'learning_rate': 0.1898680104008436, 'gamma': 0.6529928211412019}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:28:04,383] Trial 44 finished with value: 0.8384404409787576 and parameters: {'n_estimators': 213, 'max_depth': 8, 'learning_rate': 0.10810174766565014, 'gamma': 0.9254694995358991}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:28:10,767] Trial 45 finished with value: 0.8372627050282334 and parameters: {'n_estimators': 129, 'max_depth': 7, 'learning_rate': 0.2311609528077135, 'gamma': 1.2885033632334069}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:28:17,715] Trial 46 finished with value: 0.8375638612530251 and parameters: {'n_estimators': 181, 'max_depth': 8, 'learning_rate': 0.15195746654678624, 'gamma': 1.6212877797322847}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:28:24,118] Trial 47 finished with value: 0.8357085237967196 and parameters: {'n_estimators': 205, 'max_depth': 4, 'learning_rate': 0.22170625180752754, 'gamma': 0.4116836682217796}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:28:32,410] Trial 48 finished with value: 0.8330572734606078 and parameters: {'n_estimators': 236, 'max_depth': 6, 'learning_rate': 0.06227610087360175, 'gamma': 1.1858392443476917}. Best is trial 30 with value: 0.8392363538585641.\n",
      "[I 2025-11-16 16:28:43,370] Trial 49 finished with value: 0.7850067222371606 and parameters: {'n_estimators': 307, 'max_depth': 7, 'learning_rate': 0.004246281769481876, 'gamma': 1.4429702962416895}. Best is trial 30 with value: 0.8392363538585641.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB best accuracy: 0.8392363538585641\n",
      "XGB best params: {'n_estimators': 371, 'max_depth': 7, 'learning_rate': 0.1789693608288411, 'gamma': 0.766163101908516}\n"
     ]
    }
   ],
   "source": [
    "def objective_xgb(trial: optuna.Trial) -> float:\n",
    "    model_params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 400),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"eval_metric\": \"logloss\",\n",
    "    }\n",
    "\n",
    "    fold_scores = perform_CV(\n",
    "        data=kick_transformed,\n",
    "        model_class=XGBClassifier,\n",
    "        model_params=model_params,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    mean_accuracy = float(np.mean([fold[\"accuracy\"] for fold in fold_scores]))\n",
    "    return mean_accuracy\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_xgb = optuna.create_study(direction=\"maximize\", study_name=\"xgb_accuracy\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "\n",
    "print(\"XGB best accuracy:\", study_xgb.best_value)\n",
    "print(\"XGB best params:\", study_xgb.best_params)\n",
    "\n",
    "df_xgb_cat = study_xgb.trials_dataframe()\n",
    "df_xgb_cat.to_csv(\"../results/optuna_xgb_acc_trials.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b987c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 15:55:15,462] A new study created in memory with name: lr_f1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79a30f636d7425ebefd81edfc9f72cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 15:55:49,565] Trial 0 finished with value: 0.8039321655012264 and parameters: {'penalty': 'l2', 'C': 3.203430277457991}. Best is trial 0 with value: 0.8039321655012264.\n",
      "[I 2025-11-15 15:56:38,493] Trial 1 finished with value: 0.804152703664686 and parameters: {'penalty': 'l1', 'C': 0.14138073711070656}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:57:23,174] Trial 2 finished with value: 0.8040577586809678 and parameters: {'penalty': 'l1', 'C': 7.65503165689389}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:57:30,494] Trial 3 finished with value: 0.7954142533791833 and parameters: {'penalty': 'l2', 'C': 0.0019168861539635826}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:57:59,011] Trial 4 finished with value: 0.8031096344239298 and parameters: {'penalty': 'l2', 'C': 1.7295219197797516}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:58:13,234] Trial 5 finished with value: 0.7978365493619296 and parameters: {'penalty': 'l2', 'C': 0.1428805405699933}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:59:39,597] Trial 6 finished with value: 0.8040244595001959 and parameters: {'penalty': 'l1', 'C': 5.649929445115033}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:59:45,184] Trial 7 finished with value: 0.7910195281557055 and parameters: {'penalty': 'l1', 'C': 0.001515800637001579}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:00:27,904] Trial 8 finished with value: 0.8040378106912449 and parameters: {'penalty': 'l1', 'C': 9.328396589105388}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:00:55,893] Trial 9 finished with value: 0.8028580627117637 and parameters: {'penalty': 'l2', 'C': 1.3939687134583878}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:01:17,269] Trial 10 finished with value: 0.8027605227497332 and parameters: {'penalty': 'l1', 'C': 0.041152987666928625}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:01:27,261] Trial 11 finished with value: 0.8041030038985572 and parameters: {'penalty': 'l1', 'C': 86.92373267113412}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:01:41,641] Trial 12 finished with value: 0.8000652619819986 and parameters: {'penalty': 'l1', 'C': 0.02431823842419314}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:01:51,881] Trial 13 finished with value: 0.8040830178006573 and parameters: {'penalty': 'l1', 'C': 89.538752517193}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:02:01,715] Trial 14 finished with value: 0.8040896582055804 and parameters: {'penalty': 'l1', 'C': 89.90957472109224}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:03:34,508] Trial 15 finished with value: 0.8041638209883251 and parameters: {'penalty': 'l1', 'C': 0.3449980898859952}. Best is trial 15 with value: 0.8041638209883251.\n",
      "[I 2025-11-15 16:04:28,725] Trial 16 finished with value: 0.8041157557697837 and parameters: {'penalty': 'l1', 'C': 0.17440826307847038}. Best is trial 15 with value: 0.8041638209883251.\n",
      "[I 2025-11-15 16:04:40,432] Trial 17 finished with value: 0.7959220654469126 and parameters: {'penalty': 'l1', 'C': 0.013010350624506652}. Best is trial 15 with value: 0.8041638209883251.\n",
      "[I 2025-11-15 16:06:25,667] Trial 18 finished with value: 0.8041952997933223 and parameters: {'penalty': 'l1', 'C': 0.4627768924709329}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:08:22,784] Trial 19 finished with value: 0.8041710864324123 and parameters: {'penalty': 'l1', 'C': 0.5744440320924616}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:10:00,925] Trial 20 finished with value: 0.8041674346400278 and parameters: {'penalty': 'l1', 'C': 0.522533314831677}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:11:52,492] Trial 21 finished with value: 0.8041059213720759 and parameters: {'penalty': 'l1', 'C': 0.9706883132618757}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:13:45,733] Trial 22 finished with value: 0.804157260570894 and parameters: {'penalty': 'l1', 'C': 0.7830926328587215}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:15:15,988] Trial 23 finished with value: 0.804125367104621 and parameters: {'penalty': 'l1', 'C': 0.3257710014559326}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:15:44,542] Trial 24 finished with value: 0.8036584029631448 and parameters: {'penalty': 'l1', 'C': 0.0687608977102625}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:16:11,976] Trial 25 finished with value: 0.8040813094649982 and parameters: {'penalty': 'l1', 'C': 25.631324626354456}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:16:20,957] Trial 26 finished with value: 0.7951864279217113 and parameters: {'penalty': 'l2', 'C': 0.008754092037203742}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:17:47,086] Trial 27 finished with value: 0.8041556676021411 and parameters: {'penalty': 'l1', 'C': 0.507947777888864}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:19:51,040] Trial 28 finished with value: 0.80407051830296 and parameters: {'penalty': 'l1', 'C': 2.325741780695952}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:20:26,419] Trial 29 finished with value: 0.8040602007455215 and parameters: {'penalty': 'l2', 'C': 3.743635670885835}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:20:54,557] Trial 30 finished with value: 0.8038113502227603 and parameters: {'penalty': 'l1', 'C': 0.07587891757099914}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:22:54,842] Trial 31 finished with value: 0.8041936162348756 and parameters: {'penalty': 'l1', 'C': 0.4540682154201854}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:24:43,913] Trial 32 finished with value: 0.8041631413075221 and parameters: {'penalty': 'l1', 'C': 0.6377584833956377}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:25:52,431] Trial 33 finished with value: 0.804098756294135 and parameters: {'penalty': 'l1', 'C': 0.20462921853406002}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:26:26,550] Trial 34 finished with value: 0.8038884039427586 and parameters: {'penalty': 'l1', 'C': 0.08053972522046116}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:27:57,585] Trial 35 finished with value: 0.8041873608839378 and parameters: {'penalty': 'l1', 'C': 0.359732654485548}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:28:25,367] Trial 36 finished with value: 0.8029130714223202 and parameters: {'penalty': 'l2', 'C': 1.4579110489968214}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:29:36,136] Trial 37 finished with value: 0.8041192708751043 and parameters: {'penalty': 'l1', 'C': 0.21692088782308183}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:30:16,873] Trial 38 finished with value: 0.8040611604387806 and parameters: {'penalty': 'l1', 'C': 11.014955241269627}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:30:51,064] Trial 39 finished with value: 0.8039895119340817 and parameters: {'penalty': 'l2', 'C': 3.429537117656205}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:31:29,374] Trial 40 finished with value: 0.804048944665362 and parameters: {'penalty': 'l1', 'C': 0.10689027185085612}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:33:10,106] Trial 41 finished with value: 0.8041868371518321 and parameters: {'penalty': 'l1', 'C': 0.4502861094103402}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:34:54,043] Trial 42 finished with value: 0.8040978021241123 and parameters: {'penalty': 'l1', 'C': 1.0734874453585648}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:46:03,309] Trial 43 finished with value: 0.8041509175959781 and parameters: {'penalty': 'l1', 'C': 0.37949157320283755}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:48:29,926] Trial 44 finished with value: 0.8041234728917095 and parameters: {'penalty': 'l1', 'C': 1.939194494663784}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:48:47,505] Trial 45 finished with value: 0.8026115518482897 and parameters: {'penalty': 'l1', 'C': 0.03908340790375344}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:49:43,476] Trial 46 finished with value: 0.8041302954657402 and parameters: {'penalty': 'l1', 'C': 0.19125137981113388}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:50:27,415] Trial 47 finished with value: 0.8042505633259674 and parameters: {'penalty': 'l2', 'C': 5.809841990405055}. Best is trial 47 with value: 0.8042505633259674.\n",
      "[I 2025-11-15 16:51:21,354] Trial 48 finished with value: 0.8041708855043395 and parameters: {'penalty': 'l2', 'C': 35.99637092292645}. Best is trial 47 with value: 0.8042505633259674.\n",
      "[I 2025-11-15 16:52:04,785] Trial 49 finished with value: 0.8042921221317878 and parameters: {'penalty': 'l2', 'C': 5.950374724789477}. Best is trial 49 with value: 0.8042921221317878.\n",
      "LR best F1: 0.8042921221317878\n",
      "LR best params: {'penalty': 'l2', 'C': 5.950374724789477}\n"
     ]
    }
   ],
   "source": [
    "def objective_lr(trial: optuna.Trial) -> float:\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "    solver = \"liblinear\"  # supports both l1 and l2\n",
    "\n",
    "    model_params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-3, 100.0, log=True),\n",
    "        \"penalty\": penalty,\n",
    "        \"solver\": solver,\n",
    "        \"max_iter\": 1000,\n",
    "    }\n",
    "\n",
    "    fold_scores = perform_CV(\n",
    "        data=kick_transformed,\n",
    "        model_class=LogisticRegression,\n",
    "        model_params=model_params,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "    return mean_f1\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_lr = optuna.create_study(direction=\"maximize\", study_name=\"lr_f1\")\n",
    "study_lr.optimize(objective_lr, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"LR best F1:\", study_lr.best_value)\n",
    "print(\"LR best params:\", study_lr.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71400a",
   "metadata": {},
   "source": [
    "Optimization (XGBoost) Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c87fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for category: Theater\n",
      "XGB best F1 for Theater: 0.8092362504895695\n",
      "XGB best params for Theater: {'n_estimators': 293, 'max_depth': 6, 'learning_rate': 0.0034021548922418725, 'gamma': 2.0946971222798467}\n",
      "Optimizing for category: Technology\n",
      "XGB best F1 for Technology: 0.8730187080472896\n",
      "XGB best params for Technology: {'n_estimators': 348, 'max_depth': 9, 'learning_rate': 0.01877675526370727, 'gamma': 2.671432995667378}\n",
      "Optimizing for category: Dance\n",
      "XGB best F1 for Dance: 0.8038695078740385\n",
      "XGB best params for Dance: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.13188114596703313, 'gamma': 4.1070555608529915}\n",
      "Optimizing for category: Film & Video\n",
      "XGB best F1 for Film & Video: 0.8979781407635491\n",
      "XGB best params for Film & Video: {'n_estimators': 355, 'max_depth': 3, 'learning_rate': 0.23874158155646538, 'gamma': 0.9203490652325381}\n",
      "Optimizing for category: Music\n",
      "XGB best F1 for Music: 0.8963609845152269\n",
      "XGB best params for Music: {'n_estimators': 356, 'max_depth': 5, 'learning_rate': 0.03083965183551209, 'gamma': 4.242163339581606}\n",
      "Optimizing for category: Comics\n",
      "XGB best F1 for Comics: 0.9237693146302792\n",
      "XGB best params for Comics: {'n_estimators': 369, 'max_depth': 9, 'learning_rate': 0.039492757347250114, 'gamma': 1.4969443502327442}\n",
      "Optimizing for category: Publishing\n",
      "XGB best F1 for Publishing: 0.8647584706728771\n",
      "XGB best params for Publishing: {'n_estimators': 121, 'max_depth': 8, 'learning_rate': 0.05027572392306775, 'gamma': 4.589500712901272}\n",
      "Optimizing for category: Food\n",
      "XGB best F1 for Food: 0.7786354487150678\n",
      "XGB best params for Food: {'n_estimators': 222, 'max_depth': 4, 'learning_rate': 0.21568518924262597, 'gamma': 1.014468541738383}\n",
      "Optimizing for category: None\n",
      "XGB best F1 for None: 0.9832682949763745\n",
      "XGB best params for None: {'n_estimators': 347, 'max_depth': 6, 'learning_rate': 0.12887139900666725, 'gamma': 2.5652958113058646}\n",
      "Optimizing for category: Journalism\n",
      "XGB best F1 for Journalism: 0.6165156631278472\n",
      "XGB best params for Journalism: {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.08714708204977763, 'gamma': 0.8899954360013724}\n",
      "Optimizing for category: Games\n",
      "XGB best F1 for Games: 0.9127095188729382\n",
      "XGB best params for Games: {'n_estimators': 102, 'max_depth': 4, 'learning_rate': 0.08900502947497695, 'gamma': 0.9551178960876545}\n",
      "Optimizing for category: Fashion\n",
      "XGB best F1 for Fashion: 0.8362768786047482\n",
      "XGB best params for Fashion: {'n_estimators': 389, 'max_depth': 2, 'learning_rate': 0.2573850147732169, 'gamma': 0.4078747767273293}\n",
      "Optimizing for category: Art\n",
      "XGB best F1 for Art: 0.8814635850746804\n",
      "XGB best params for Art: {'n_estimators': 347, 'max_depth': 7, 'learning_rate': 0.11132637540148047, 'gamma': 0.6917612555184253}\n",
      "Optimizing for category: Design\n",
      "XGB best F1 for Design: 0.8573381754835537\n",
      "XGB best params for Design: {'n_estimators': 375, 'max_depth': 3, 'learning_rate': 0.15035672496825442, 'gamma': 1.3173785478574964}\n",
      "Optimizing for category: Photography\n",
      "XGB best F1 for Photography: 0.801516623777669\n",
      "XGB best params for Photography: {'n_estimators': 329, 'max_depth': 3, 'learning_rate': 0.04629566181566946, 'gamma': 0.5307260253721594}\n",
      "Optimizing for category: Crafts\n",
      "XGB best F1 for Crafts: 0.6453443327766906\n",
      "XGB best params for Crafts: {'n_estimators': 225, 'max_depth': 4, 'learning_rate': 0.2160719764246308, 'gamma': 0.30308397721791874}\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "\n",
    "best_xgb_cat_results = []\n",
    "for cat_parent_name in kick_transformed['cat_parent_name'].unique():\n",
    "    print(f\"Optimizing for category: {cat_parent_name}\")\n",
    "\n",
    "    def objective_cat(trial: optuna.Trial) -> float:\n",
    "        model_params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 400),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"eval_metric\": \"logloss\",\n",
    "        }\n",
    "\n",
    "        fold_scores = perform_CV(\n",
    "            data=kick_transformed[kick_transformed['cat_parent_name'] == cat_parent_name],\n",
    "            model_class=XGBClassifier,\n",
    "            model_params=model_params,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "        mean_acc = float(np.mean([fold[\"accuracy\"] for fold in fold_scores]))\n",
    "        # return mean_f1\n",
    "        return mean_acc\n",
    "\n",
    "    study_cat = optuna.create_study(direction=\"maximize\", study_name=f\"xgb_f1_{cat_parent_name}\")\n",
    "    study_cat.optimize(objective_cat, n_trials=100)\n",
    "\n",
    "    print(f\"XGB best F1 for {cat_parent_name}:\", study_cat.best_value)\n",
    "    print(f\"XGB best params for {cat_parent_name}:\", study_cat.best_params)\n",
    "    best_xgb_cat_results.append({\n",
    "        \"cat_parent_name\": cat_parent_name,\n",
    "        # \"best_f1\": study_cat.best_value,\n",
    "        \"best_accuracy\": study_cat.best_value,\n",
    "        \"best_params\": study_cat.best_params\n",
    "    })\n",
    "    \n",
    "    # XGBoost study\n",
    "    df_xgb_cat = study_cat.trials_dataframe()\n",
    "    df_xgb_cat.to_csv(f\"../results/model_results/xgb_categorical/optuna_{cat_parent_name}_xgb_trials.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a17bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating best XGB model for category: Theater\n",
      "Mean F1 for Theater with best XGB params: 0.7886\n",
      "Evaluating best XGB model for category: Technology\n",
      "Mean F1 for Technology with best XGB params: 0.8672\n",
      "Evaluating best XGB model for category: Dance\n",
      "Mean F1 for Dance with best XGB params: 0.7775\n",
      "Evaluating best XGB model for category: Film & Video\n",
      "Mean F1 for Film & Video with best XGB params: 0.8935\n",
      "Evaluating best XGB model for category: Music\n",
      "Mean F1 for Music with best XGB params: 0.8928\n",
      "Evaluating best XGB model for category: Comics\n",
      "Mean F1 for Comics with best XGB params: 0.9228\n",
      "Evaluating best XGB model for category: Publishing\n",
      "Mean F1 for Publishing with best XGB params: 0.8610\n",
      "Evaluating best XGB model for category: Food\n",
      "Mean F1 for Food with best XGB params: 0.7682\n",
      "Evaluating best XGB model for category: None\n",
      "Mean F1 for None with best XGB params: 0.9828\n",
      "Evaluating best XGB model for category: Journalism\n",
      "Mean F1 for Journalism with best XGB params: 0.6016\n",
      "Evaluating best XGB model for category: Games\n",
      "Mean F1 for Games with best XGB params: 0.9091\n",
      "Evaluating best XGB model for category: Fashion\n",
      "Mean F1 for Fashion with best XGB params: 0.8263\n",
      "Evaluating best XGB model for category: Art\n",
      "Mean F1 for Art with best XGB params: 0.8772\n",
      "Evaluating best XGB model for category: Design\n",
      "Mean F1 for Design with best XGB params: 0.8398\n",
      "Evaluating best XGB model for category: Photography\n",
      "Mean F1 for Photography with best XGB params: 0.7908\n",
      "Evaluating best XGB model for category: Crafts\n",
      "Mean F1 for Crafts with best XGB params: 0.6304\n"
     ]
    }
   ],
   "source": [
    "xgb_results_df = pd.read_csv(\"../results/optuna_xgb_trials.csv\")\n",
    "\n",
    "# Extract parameters of the best trial\n",
    "best_trial = xgb_results_df.loc[xgb_results_df['value'].idxmax()]\n",
    "xgb_best_params = {\n",
    "    'n_estimators': int(best_trial['params_n_estimators']),\n",
    "    'max_depth': int(best_trial['params_max_depth']),\n",
    "    'learning_rate': float(best_trial['params_learning_rate']),\n",
    "    'gamma': float(best_trial['params_gamma']),\n",
    "    'eval_metric': 'logloss',\n",
    "}\n",
    "xgb_model = XGBClassifier(**xgb_best_params)\n",
    "\n",
    "# Use perform_CV on categorical subsets using best params\n",
    "for cat_parent_name in kick_transformed['cat_parent_name'].unique():\n",
    "    print(f\"Evaluating best XGB model for category: {cat_parent_name}\")\n",
    "    fold_scores = perform_CV(\n",
    "        data=kick_transformed[kick_transformed['cat_parent_name'] == cat_parent_name],\n",
    "        model_class=XGBClassifier,\n",
    "        model_params=xgb_best_params,\n",
    "        verbose=False,\n",
    "    )\n",
    "    mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "    print(f\"Mean F1 for {cat_parent_name} with best XGB params: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7af6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "\taccuracy=0.8408, \n",
      "\tprecision=0.8736, \n",
      "\trecall=0.8692, \n",
      "\tf1=0.8714\n",
      "\n",
      "Fold 2\n",
      "\taccuracy=0.8417, \n",
      "\tprecision=0.8721, \n",
      "\trecall=0.8730, \n",
      "\tf1=0.8725\n",
      "\n",
      "Fold 3\n",
      "\taccuracy=0.8395, \n",
      "\tprecision=0.8691, \n",
      "\trecall=0.8728, \n",
      "\tf1=0.8710\n",
      "\n",
      "Fold 4\n",
      "\taccuracy=0.8372, \n",
      "\tprecision=0.8669, \n",
      "\trecall=0.8716, \n",
      "\tf1=0.8692\n",
      "\n",
      "Fold 5\n",
      "\taccuracy=0.8379, \n",
      "\tprecision=0.8700, \n",
      "\trecall=0.8685, \n",
      "\tf1=0.8693\n",
      "Average F1 across all data with ensemble XGB: 0.8707\n"
     ]
    }
   ],
   "source": [
    "def perform_CV_xgb_ensemble(\n",
    "        data: pd.DataFrame,\n",
    "        verbose = False\n",
    "    ) -> list[dict[str, float]]:\n",
    "    df = data.copy()\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    numerical_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    train = df.drop(columns=[\"is_successful\"])\n",
    "    target = df[\"is_successful\"]\n",
    "\n",
    "    train[numerical_cols] = train[numerical_cols].astype(\"float64\")\n",
    "\n",
    "    # ====================================\n",
    "    #        CROSS-VALIDATION LOOP\n",
    "    # ====================================\n",
    "    fold_scores = []\n",
    "    skf = StratifiedKFold(n_splits=K_splits, shuffle=True, random_state=random_state)\n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(train, target)):\n",
    "        if verbose:\n",
    "            print(f\"\\nFold {i+1}\")\n",
    "\n",
    "        X_train, X_test = train.iloc[train_idx], train.iloc[test_idx]\n",
    "        y_train, y_test = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "        # container for predictions on the full test fold\n",
    "        y_pred_full = pd.Series(index=X_test.index, dtype=int)\n",
    "        \n",
    "        # ====================================\n",
    "        #           TRAIN AND EVALUTATE (ONE PER CATEGORY PARENT)\n",
    "        # ====================================\n",
    "        for cat_parent_name in data['cat_parent_name'].unique():\n",
    "            # get data for this category\n",
    "            train_mask = (X_train['cat_parent_name'] == cat_parent_name)\n",
    "            test_mask = (X_test['cat_parent_name'] == cat_parent_name)\n",
    "            X_train_cat = X_train[train_mask]\n",
    "            y_train_cat = y_train[train_mask]\n",
    "            X_test_cat = X_test[test_mask]\n",
    "            # y_test_cat = y_test[test_mask]\n",
    "\n",
    "            # get best params:\n",
    "            best_params_df = pd.read_csv(f\"../results/model_results/xgb_categorical/optuna_{cat_parent_name}_xgb_trials.csv\")\n",
    "            best_trial = best_params_df.loc[best_params_df['value'].idxmax()]\n",
    "            best_params = {\n",
    "                'n_estimators': int(best_trial['params_n_estimators']),\n",
    "                'max_depth': int(best_trial['params_max_depth']),\n",
    "                'learning_rate': float(best_trial['params_learning_rate']),\n",
    "                'gamma': float(best_trial['params_gamma']),\n",
    "                'eval_metric': 'logloss',\n",
    "            }\n",
    "\n",
    "            # preprocess per category (fit on cat-train, apply to cat-test)\n",
    "            X_train_proc, X_test_proc = preprocess_data(\n",
    "                X_train_cat,\n",
    "                X_test_cat,\n",
    "                categorical_cols=categorical_cols,\n",
    "                numerical_cols=numerical_cols,\n",
    "            )\n",
    "\n",
    "            model = XGBClassifier(**best_params)\n",
    "            model.fit(X_train_proc, y_train_cat)\n",
    "\n",
    "            y_pred_cat = model.predict(X_test_proc)\n",
    "\n",
    "            # write predictions back into the global y_pred_full\n",
    "            y_pred_full.loc[test_mask] = y_pred_cat\n",
    "\n",
    "        # Now evaluate ensemble on this fold\n",
    "        y_true_fold = y_test\n",
    "        y_pred_fold = y_pred_full\n",
    "\n",
    "        fold_result = {\n",
    "            \"accuracy\": accuracy_score(y_true_fold, y_pred_fold),\n",
    "            \"precision\": precision_score(y_true_fold, y_pred_fold, zero_division=0),\n",
    "            \"recall\": recall_score(y_true_fold, y_pred_fold, zero_division=0),\n",
    "            \"f1\": f1_score(y_true_fold, y_pred_fold, zero_division=0),\n",
    "        }\n",
    "        fold_scores.append(fold_result)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\taccuracy={fold_result['accuracy']:.4f}, \\n\"\n",
    "                f\"\\tprecision={fold_result['precision']:.4f}, \\n\"\n",
    "                f\"\\trecall={fold_result['recall']:.4f}, \\n\"\n",
    "                f\"\\tf1={fold_result['f1']:.4f}\"\n",
    "            )\n",
    "\n",
    "    return fold_scores\n",
    "\n",
    "fold_scores = perform_CV_xgb_ensemble(kick_transformed, verbose=True)\n",
    "# get average f1 score across folds\n",
    "mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "print(f\"Average F1 across all data with ensemble XGB: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df5a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\taccuracy=0.8403, \n",
      "\tprecision=0.8718, \n",
      "\trecall=0.8708, \n",
      "\tf1=0.8713\n",
      "Fold 2\n",
      "\taccuracy=0.8417, \n",
      "\tprecision=0.8679, \n",
      "\trecall=0.8787, \n",
      "\tf1=0.8733\n",
      "Fold 3\n",
      "\taccuracy=0.8392, \n",
      "\tprecision=0.8676, \n",
      "\trecall=0.8744, \n",
      "\tf1=0.8710\n",
      "Fold 4\n",
      "\taccuracy=0.8373, \n",
      "\tprecision=0.8661, \n",
      "\trecall=0.8728, \n",
      "\tf1=0.8694\n",
      "Fold 5\n",
      "\taccuracy=0.8380, \n",
      "\tprecision=0.8681, \n",
      "\trecall=0.8715, \n",
      "\tf1=0.8698\n",
      "Average F1 across all data with best XGB params: 0.8710\n"
     ]
    }
   ],
   "source": [
    "fold_scores = perform_CV(kick_transformed, XGBClassifier, xgb_best_params, verbose=True)\n",
    "# get average f1 score across folds\n",
    "\n",
    "\n",
    "mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "print(f\"Average F1 across all data with best XGB params: {mean_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMSE802_PROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
