{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e89be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.utils import load_processed_data\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9f3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "kick_clean = load_processed_data(use_relative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad84ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_final_irrelevant_columns_and_encode_target(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # These are useful for lookup / human insight, but not for machine learning\n",
    "    df = df.copy()\n",
    "    irrelevant_columns = [\n",
    "        \"index\",\n",
    "        \"name\",\n",
    "        \"blurb\",\n",
    "    ]\n",
    "    df = df.drop(columns=irrelevant_columns)\n",
    "\n",
    "    # Encode target\n",
    "    df[\"is_successful\"] = df[\"state\"] == \"successful\"\n",
    "    df = df.drop(columns=[\"state\"])\n",
    "    return df\n",
    "\n",
    "def handle_datetime_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    datetime_columns = [\n",
    "        \"created_at\",\n",
    "        \"deadline\",\n",
    "        \"launched_at\"\n",
    "    ]\n",
    "\n",
    "    df[\"age_days\"] = (pd.Timestamp.now() - df[\"launched_at\"]).dt.days\n",
    "\n",
    "    df = df.drop(columns = datetime_columns)\n",
    "    return df\n",
    "\n",
    "def machine_ready_preprocessing(\n",
    "    df: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, LabelEncoder, StandardScaler]:\n",
    "    df = handle_datetime_features(df)\n",
    "    df = drop_final_irrelevant_columns_and_encode_target(df)\n",
    "    return df\n",
    "\n",
    "kick_transformed = machine_ready_preprocessing(kick_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "#               CONFIG\n",
    "# ====================================\n",
    "K_splits = 5\n",
    "random_state = 1337\n",
    "\n",
    "def preprocess_data(X_train, X_test, categorical_cols, numerical_cols):\n",
    "    # Training set will fit the encoders / scalers, the test set will use the fitted encoders / scalers\n",
    "    # ====================================\n",
    "    #        CATEGORICAL ENCODING\n",
    "    # ====================================\n",
    "    # Encode categorical features\n",
    "    # OneHotEncoding for features that have relatively low cardinality (< 100 unique values)\n",
    "    # FrequencyEncoding for features that have relatively high cardinality (>= 100 unique values)\n",
    "    for cat_col in categorical_cols:\n",
    "        if X_train[cat_col].nunique() < 100: # OneHotEncoding for low cardinality\n",
    "            oh_encoder = OneHotEncoder(\n",
    "                handle_unknown='ignore',\n",
    "                sparse_output=False\n",
    "            )\n",
    "            oh_encoder.fit(X_train[[cat_col]])\n",
    "\n",
    "            # Transform and join back to dataframe\n",
    "            X_train_oh = pd.DataFrame(\n",
    "                oh_encoder.transform(X_train[[cat_col]]),\n",
    "                columns=[f\"{cat_col}__{cat}\" for cat in oh_encoder.categories_[0]],\n",
    "                index=X_train.index\n",
    "            )\n",
    "            X_test_oh = pd.DataFrame(\n",
    "                oh_encoder.transform(X_test[[cat_col]]),\n",
    "                columns=[f\"{cat_col}__{cat}\" for cat in oh_encoder.categories_[0]],\n",
    "                index=X_test.index\n",
    "            )\n",
    "\n",
    "            X_train = X_train.drop(columns=[cat_col]).join(X_train_oh)\n",
    "            X_test = X_test.drop(columns=[cat_col]).join(X_test_oh)\n",
    "        else: # FrequencyEncoding for high cardinality\n",
    "            freq_encoding = X_train[cat_col].value_counts(normalize=True)\n",
    "            X_train.loc[:, cat_col] = X_train[cat_col].map(freq_encoding)\n",
    "            X_test.loc[:, cat_col] = X_test[cat_col].map(freq_encoding).fillna(1 / (X_train.__len__() + X_test.__len__())) # Fill unseen with \"rare\" frequency\n",
    "            X_train[cat_col] = X_train[cat_col].astype(\"float64\")\n",
    "            X_test[cat_col] = X_test[cat_col].astype(\"float64\")\n",
    "    \n",
    "    # ====================================\n",
    "    #         NUMERICAL SCALING\n",
    "    # ====================================\n",
    "\n",
    "    num_scaler = StandardScaler()\n",
    "    num_scaler.fit(X_train[numerical_cols])\n",
    "    X_train.loc[:, numerical_cols] = num_scaler.transform(X_train[numerical_cols])\n",
    "    X_test.loc[:, numerical_cols] = num_scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def perform_CV(\n",
    "        data: pd.DataFrame, \n",
    "        model_class, \n",
    "        model_params, \n",
    "        verbose = False\n",
    "    ) -> list[dict[str, float]]:\n",
    "    df = data.copy()\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    numerical_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    train = df.drop(columns=[\"is_successful\"])\n",
    "    target = df[\"is_successful\"]\n",
    "\n",
    "    # Ensure numerical columns are float64\n",
    "    train[numerical_cols] = train[numerical_cols].astype(\"float64\")\n",
    "\n",
    "    # ====================================\n",
    "    #        CROSS-VALIDATION LOOP\n",
    "    # ====================================\n",
    "    fold_scores = []\n",
    "    sk_fold = StratifiedKFold(n_splits=K_splits, shuffle=True, random_state=random_state)\n",
    "    for i, (train_idx, test_idx) in enumerate(sk_fold.split(train, target)):\n",
    "        if verbose:\n",
    "            print(f\"Fold {i+1}\")\n",
    "\n",
    "        X_train, X_test = train.iloc[train_idx], train.iloc[test_idx]\n",
    "        y_train, y_test = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "        # APPLY PREPROCESSING\n",
    "        X_train, X_test = preprocess_data(X_train, X_test, categorical_cols, numerical_cols)\n",
    "\n",
    "        # ====================================\n",
    "        #         TRAIN AND EVALUTATE\n",
    "        # ====================================\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        fold_result = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        }\n",
    "        fold_scores.append(fold_result)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\taccuracy={fold_result['accuracy']:.4f}, \\n\"\n",
    "                f\"\\tprecision={fold_result['precision']:.4f}, \\n\"\n",
    "                f\"\\trecall={fold_result['recall']:.4f}, \\n\"\n",
    "                f\"\\tf1={fold_result['f1']:.4f}\"\n",
    "            )\n",
    "    return fold_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27983bac",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 13:06:49,960] A new study created in memory with name: rf_f1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1b8a79761c495a85154a0ff47465a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 13:07:37,543] Trial 0 finished with value: 0.8561253620589275 and parameters: {'n_estimators': 73, 'max_depth': 16, 'min_samples_split': 19}. Best is trial 0 with value: 0.8561253620589275.\n",
      "[I 2025-11-15 13:08:39,559] Trial 1 finished with value: 0.8561951365244977 and parameters: {'n_estimators': 96, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 1 with value: 0.8561951365244977.\n",
      "[I 2025-11-15 13:08:49,278] Trial 2 finished with value: 0.7929651720133478 and parameters: {'n_estimators': 42, 'max_depth': 3, 'min_samples_split': 13}. Best is trial 1 with value: 0.8561951365244977.\n",
      "[I 2025-11-15 13:09:58,568] Trial 3 finished with value: 0.8639876440169967 and parameters: {'n_estimators': 73, 'max_depth': 30, 'min_samples_split': 5}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:10:19,882] Trial 4 finished with value: 0.8537609609619728 and parameters: {'n_estimators': 29, 'max_depth': 16, 'min_samples_split': 14}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:11:04,568] Trial 5 finished with value: 0.8617206193181317 and parameters: {'n_estimators': 52, 'max_depth': 24, 'min_samples_split': 16}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:11:15,443] Trial 6 finished with value: 0.8501127002128015 and parameters: {'n_estimators': 11, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:11:50,629] Trial 7 finished with value: 0.8424634458747648 and parameters: {'n_estimators': 89, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:12:04,571] Trial 8 finished with value: 0.8537428031914056 and parameters: {'n_estimators': 16, 'max_depth': 17, 'min_samples_split': 10}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:12:56,231] Trial 9 finished with value: 0.8598765061959106 and parameters: {'n_estimators': 67, 'max_depth': 20, 'min_samples_split': 14}. Best is trial 3 with value: 0.8639876440169967.\n",
      "[I 2025-11-15 13:14:07,364] Trial 10 finished with value: 0.8641887512385175 and parameters: {'n_estimators': 76, 'max_depth': 30, 'min_samples_split': 6}. Best is trial 10 with value: 0.8641887512385175.\n",
      "[I 2025-11-15 13:15:17,143] Trial 11 finished with value: 0.8644150198031266 and parameters: {'n_estimators': 75, 'max_depth': 29, 'min_samples_split': 6}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:16:35,473] Trial 12 finished with value: 0.8640950104144276 and parameters: {'n_estimators': 86, 'max_depth': 28, 'min_samples_split': 7}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:17:26,814] Trial 13 finished with value: 0.862527581902742 and parameters: {'n_estimators': 57, 'max_depth': 25, 'min_samples_split': 7}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:18:40,565] Trial 14 finished with value: 0.8636111513687407 and parameters: {'n_estimators': 81, 'max_depth': 25, 'min_samples_split': 2}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:19:35,273] Trial 15 finished with value: 0.8631469772877052 and parameters: {'n_estimators': 58, 'max_depth': 30, 'min_samples_split': 7}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:20:57,522] Trial 16 finished with value: 0.8628021463132216 and parameters: {'n_estimators': 99, 'max_depth': 22, 'min_samples_split': 4}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:21:19,203] Trial 17 finished with value: 0.8459429057146336 and parameters: {'n_estimators': 42, 'max_depth': 11, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:22:21,553] Trial 18 finished with value: 0.8639616011212755 and parameters: {'n_estimators': 69, 'max_depth': 27, 'min_samples_split': 8}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:23:26,706] Trial 19 finished with value: 0.86101066232453 and parameters: {'n_estimators': 83, 'max_depth': 21, 'min_samples_split': 12}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:24:11,098] Trial 20 finished with value: 0.861887902711221 and parameters: {'n_estimators': 47, 'max_depth': 27, 'min_samples_split': 4}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:25:28,590] Trial 21 finished with value: 0.8643029084862388 and parameters: {'n_estimators': 84, 'max_depth': 30, 'min_samples_split': 6}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:26:39,454] Trial 22 finished with value: 0.8641108196160339 and parameters: {'n_estimators': 77, 'max_depth': 29, 'min_samples_split': 6}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:27:39,440] Trial 23 finished with value: 0.8631800648135679 and parameters: {'n_estimators': 65, 'max_depth': 26, 'min_samples_split': 3}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:29:00,945] Trial 24 finished with value: 0.864321936700031 and parameters: {'n_estimators': 90, 'max_depth': 30, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:30:17,668] Trial 25 finished with value: 0.8632067104254778 and parameters: {'n_estimators': 93, 'max_depth': 23, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:31:37,323] Trial 26 finished with value: 0.8642582423059775 and parameters: {'n_estimators': 90, 'max_depth': 28, 'min_samples_split': 11}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:32:39,781] Trial 27 finished with value: 0.8606793156508676 and parameters: {'n_estimators': 82, 'max_depth': 20, 'min_samples_split': 8}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:32:57,417] Trial 28 finished with value: 0.7927264733532887 and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:33:25,776] Trial 29 finished with value: 0.8439892209268823 and parameters: {'n_estimators': 64, 'max_depth': 10, 'min_samples_split': 17}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:58:11,326] Trial 30 finished with value: 0.8625552199351487 and parameters: {'n_estimators': 78, 'max_depth': 26, 'min_samples_split': 19}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 13:59:32,571] Trial 31 finished with value: 0.8640123869154799 and parameters: {'n_estimators': 89, 'max_depth': 28, 'min_samples_split': 11}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:00:55,289] Trial 32 finished with value: 0.8637176261684294 and parameters: {'n_estimators': 93, 'max_depth': 28, 'min_samples_split': 11}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:02:20,544] Trial 33 finished with value: 0.8643495286943541 and parameters: {'n_estimators': 93, 'max_depth': 30, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:03:46,902] Trial 34 finished with value: 0.8643413369219319 and parameters: {'n_estimators': 95, 'max_depth': 30, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:04:53,927] Trial 35 finished with value: 0.8643310031240041 and parameters: {'n_estimators': 72, 'max_depth': 30, 'min_samples_split': 8}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:05:56,495] Trial 36 finished with value: 0.8636627949991468 and parameters: {'n_estimators': 73, 'max_depth': 25, 'min_samples_split': 13}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:06:47,673] Trial 37 finished with value: 0.8509424757574218 and parameters: {'n_estimators': 96, 'max_depth': 13, 'min_samples_split': 8}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:07:40,679] Trial 38 finished with value: 0.8622236487010178 and parameters: {'n_estimators': 62, 'max_depth': 24, 'min_samples_split': 10}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:08:45,304] Trial 39 finished with value: 0.8631195505752366 and parameters: {'n_estimators': 71, 'max_depth': 27, 'min_samples_split': 5}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:09:03,286] Trial 40 finished with value: 0.8352527417407991 and parameters: {'n_estimators': 51, 'max_depth': 7, 'min_samples_split': 12}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:10:27,546] Trial 41 finished with value: 0.8642536432679677 and parameters: {'n_estimators': 94, 'max_depth': 30, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:11:44,962] Trial 42 finished with value: 0.8642351580819267 and parameters: {'n_estimators': 86, 'max_depth': 29, 'min_samples_split': 9}. Best is trial 11 with value: 0.8644150198031266.\n",
      "[I 2025-11-15 14:13:05,402] Trial 43 finished with value: 0.8646448835621587 and parameters: {'n_estimators': 89, 'max_depth': 29, 'min_samples_split': 8}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 14:14:14,149] Trial 44 finished with value: 0.864357024386979 and parameters: {'n_estimators': 75, 'max_depth': 29, 'min_samples_split': 7}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 14:14:39,595] Trial 45 finished with value: 0.8596903792862653 and parameters: {'n_estimators': 26, 'max_depth': 26, 'min_samples_split': 7}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 14:15:47,359] Trial 46 finished with value: 0.8631953224233808 and parameters: {'n_estimators': 80, 'max_depth': 23, 'min_samples_split': 5}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 15:48:32,399] Trial 47 finished with value: 0.864159981775001 and parameters: {'n_estimators': 99, 'max_depth': 28, 'min_samples_split': 10}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 15:49:52,594] Trial 48 finished with value: 0.8641860766582352 and parameters: {'n_estimators': 87, 'max_depth': 29, 'min_samples_split': 6}. Best is trial 43 with value: 0.8646448835621587.\n",
      "[I 2025-11-15 15:51:16,345] Trial 49 finished with value: 0.8632952688974985 and parameters: {'n_estimators': 97, 'max_depth': 24, 'min_samples_split': 7}. Best is trial 43 with value: 0.8646448835621587.\n",
      "RF best F1: 0.8646448835621587\n",
      "RF best params: {'n_estimators': 89, 'max_depth': 29, 'min_samples_split': 8}\n"
     ]
    }
   ],
   "source": [
    "def objective_rf(trial: optuna.Trial) -> float:\n",
    "    model_params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        # \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "    }\n",
    "\n",
    "    fold_scores = perform_CV(\n",
    "        data=kick_transformed,\n",
    "        model_class=RandomForestClassifier,\n",
    "        model_params=model_params,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "    return mean_f1\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_rf = optuna.create_study(direction=\"maximize\", study_name=\"rf_f1\")\n",
    "study_rf.optimize(objective_rf, n_trials=50)\n",
    "\n",
    "print(\"RF best F1:\", study_rf.best_value)\n",
    "print(\"RF best params:\", study_rf.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a4ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 10:16:31,187] A new study created in memory with name: xgb_f1\n",
      "[I 2025-11-16 10:16:39,105] Trial 0 finished with value: 0.8508975429288291 and parameters: {'n_estimators': 316, 'max_depth': 7, 'learning_rate': 0.009961344023082274, 'gamma': 0.6409944022450786}. Best is trial 0 with value: 0.8508975429288291.\n",
      "[I 2025-11-16 10:16:48,158] Trial 1 finished with value: 0.8703135545847209 and parameters: {'n_estimators': 361, 'max_depth': 12, 'learning_rate': 0.06058812302453238, 'gamma': 1.8448862663032084}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:16:57,427] Trial 2 finished with value: 0.8697249577107012 and parameters: {'n_estimators': 261, 'max_depth': 12, 'learning_rate': 0.04202422869214421, 'gamma': 2.478330462923914}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:17:06,125] Trial 3 finished with value: 0.8388865250622054 and parameters: {'n_estimators': 277, 'max_depth': 8, 'learning_rate': 0.0033877565384754075, 'gamma': 0.334712642807522}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:17:17,478] Trial 4 finished with value: 0.8508749285455973 and parameters: {'n_estimators': 383, 'max_depth': 9, 'learning_rate': 0.0038283125333185763, 'gamma': 3.9625389492332213}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:17:25,029] Trial 5 finished with value: 0.8599671527341417 and parameters: {'n_estimators': 141, 'max_depth': 10, 'learning_rate': 0.0209541538244382, 'gamma': 1.2530815089037333}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:17:31,401] Trial 6 finished with value: 0.8684651942955434 and parameters: {'n_estimators': 375, 'max_depth': 8, 'learning_rate': 0.08057060209019073, 'gamma': 4.533945344322848}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:17:36,750] Trial 7 finished with value: 0.8674766138175716 and parameters: {'n_estimators': 129, 'max_depth': 7, 'learning_rate': 0.09421617785437847, 'gamma': 3.1770507535322268}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:17:41,504] Trial 8 finished with value: 0.765972837411766 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.0019329661275063175, 'gamma': 2.288460479132807}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:17:47,224] Trial 9 finished with value: 0.8145197898345767 and parameters: {'n_estimators': 324, 'max_depth': 2, 'learning_rate': 0.0054111918908707835, 'gamma': 1.9574968136368143}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:17:52,664] Trial 10 finished with value: 0.8670516901471552 and parameters: {'n_estimators': 196, 'max_depth': 12, 'learning_rate': 0.2848039746063671, 'gamma': 3.3475957635788713}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:18:04,061] Trial 11 finished with value: 0.870047536687224 and parameters: {'n_estimators': 236, 'max_depth': 12, 'learning_rate': 0.03348475065437254, 'gamma': 1.5706432222052604}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:18:13,670] Trial 12 finished with value: 0.8687259085203243 and parameters: {'n_estimators': 200, 'max_depth': 11, 'learning_rate': 0.032618460962079335, 'gamma': 1.393514118300802}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:18:19,654] Trial 13 finished with value: 0.8690499524650758 and parameters: {'n_estimators': 236, 'max_depth': 10, 'learning_rate': 0.18594841952013055, 'gamma': 1.4466000972351605}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:18:24,141] Trial 14 finished with value: 0.8160510556816872 and parameters: {'n_estimators': 52, 'max_depth': 5, 'learning_rate': 0.013611597285594659, 'gamma': 3.135839553116419}. Best is trial 1 with value: 0.8703135545847209.\n",
      "[I 2025-11-16 10:18:34,939] Trial 15 finished with value: 0.8703655466505831 and parameters: {'n_estimators': 313, 'max_depth': 12, 'learning_rate': 0.062029823074481245, 'gamma': 0.7123459297882044}. Best is trial 15 with value: 0.8703655466505831.\n",
      "[I 2025-11-16 10:18:42,792] Trial 16 finished with value: 0.8707458677250294 and parameters: {'n_estimators': 331, 'max_depth': 10, 'learning_rate': 0.09076786804044201, 'gamma': 0.9123953242958285}. Best is trial 16 with value: 0.8707458677250294.\n",
      "[I 2025-11-16 10:18:49,626] Trial 17 finished with value: 0.870283952331285 and parameters: {'n_estimators': 316, 'max_depth': 9, 'learning_rate': 0.13983141959852188, 'gamma': 0.7590439194491131}. Best is trial 16 with value: 0.8707458677250294.\n",
      "[I 2025-11-16 10:19:00,406] Trial 18 finished with value: 0.870184816556308 and parameters: {'n_estimators': 344, 'max_depth': 10, 'learning_rate': 0.14270996250864115, 'gamma': 0.03161216329356775}. Best is trial 16 with value: 0.8707458677250294.\n",
      "[I 2025-11-16 10:19:15,189] Trial 19 finished with value: 0.8121915736867698 and parameters: {'n_estimators': 287, 'max_depth': 11, 'learning_rate': 0.0010966853808766766, 'gamma': 0.8754260407511382}. Best is trial 16 with value: 0.8707458677250294.\n",
      "[I 2025-11-16 10:19:21,869] Trial 20 finished with value: 0.8691791751994191 and parameters: {'n_estimators': 293, 'max_depth': 6, 'learning_rate': 0.2910072882810044, 'gamma': 0.07539768777783329}. Best is trial 16 with value: 0.8707458677250294.\n",
      "[I 2025-11-16 10:19:31,228] Trial 21 finished with value: 0.8704897328252998 and parameters: {'n_estimators': 364, 'max_depth': 11, 'learning_rate': 0.0566729058066332, 'gamma': 1.9059592031331232}. Best is trial 16 with value: 0.8707458677250294.\n",
      "[I 2025-11-16 10:19:42,458] Trial 22 finished with value: 0.8708991435778985 and parameters: {'n_estimators': 395, 'max_depth': 11, 'learning_rate': 0.053156148817057006, 'gamma': 0.9412001613980656}. Best is trial 22 with value: 0.8708991435778985.\n",
      "[I 2025-11-16 10:19:58,214] Trial 23 finished with value: 0.870332022787862 and parameters: {'n_estimators': 385, 'max_depth': 11, 'learning_rate': 0.02464094956774134, 'gamma': 1.1356737251684599}. Best is trial 22 with value: 0.8708991435778985.\n",
      "[I 2025-11-16 10:20:07,314] Trial 24 finished with value: 0.8700247145760664 and parameters: {'n_estimators': 400, 'max_depth': 9, 'learning_rate': 0.04779553191669282, 'gamma': 2.1008679215988755}. Best is trial 22 with value: 0.8708991435778985.\n",
      "[I 2025-11-16 10:20:14,202] Trial 25 finished with value: 0.8695543564972779 and parameters: {'n_estimators': 344, 'max_depth': 10, 'learning_rate': 0.09901060177434812, 'gamma': 2.655664797358578}. Best is trial 22 with value: 0.8708991435778985.\n",
      "[I 2025-11-16 10:20:28,813] Trial 26 finished with value: 0.8661807587236184 and parameters: {'n_estimators': 351, 'max_depth': 11, 'learning_rate': 0.013160443560783488, 'gamma': 1.7688932320570006}. Best is trial 22 with value: 0.8708991435778985.\n",
      "[I 2025-11-16 10:20:35,226] Trial 27 finished with value: 0.870167375850636 and parameters: {'n_estimators': 397, 'max_depth': 8, 'learning_rate': 0.1618700286630721, 'gamma': 1.117953783850243}. Best is trial 22 with value: 0.8708991435778985.\n",
      "[I 2025-11-16 10:20:44,990] Trial 28 finished with value: 0.8715525843921647 and parameters: {'n_estimators': 350, 'max_depth': 9, 'learning_rate': 0.07105947415092645, 'gamma': 0.4249721298819523}. Best is trial 28 with value: 0.8715525843921647.\n",
      "[I 2025-11-16 10:20:52,835] Trial 29 finished with value: 0.8711165025973429 and parameters: {'n_estimators': 329, 'max_depth': 7, 'learning_rate': 0.10057985907972466, 'gamma': 0.39608510725816526}. Best is trial 28 with value: 0.8715525843921647.\n",
      "[I 2025-11-16 10:21:00,745] Trial 30 finished with value: 0.8645215494580129 and parameters: {'n_estimators': 302, 'max_depth': 7, 'learning_rate': 0.028169169330136534, 'gamma': 0.36302735729772645}. Best is trial 28 with value: 0.8715525843921647.\n",
      "[I 2025-11-16 10:21:08,028] Trial 31 finished with value: 0.8706647117646206 and parameters: {'n_estimators': 331, 'max_depth': 6, 'learning_rate': 0.09774426249855571, 'gamma': 0.5270795884932667}. Best is trial 28 with value: 0.8715525843921647.\n",
      "[I 2025-11-16 10:21:16,804] Trial 32 finished with value: 0.8709140362530958 and parameters: {'n_estimators': 370, 'max_depth': 9, 'learning_rate': 0.06930553834824568, 'gamma': 0.9049341339151789}. Best is trial 28 with value: 0.8715525843921647.\n",
      "[I 2025-11-16 10:21:26,442] Trial 33 finished with value: 0.8704333996111778 and parameters: {'n_estimators': 363, 'max_depth': 8, 'learning_rate': 0.04398545994100077, 'gamma': 0.34328782476141645}. Best is trial 28 with value: 0.8715525843921647.\n",
      "[I 2025-11-16 10:21:33,284] Trial 34 finished with value: 0.8424112600217526 and parameters: {'n_estimators': 269, 'max_depth': 6, 'learning_rate': 0.008228242512457726, 'gamma': 0.4952931472511154}. Best is trial 28 with value: 0.8715525843921647.\n",
      "[I 2025-11-16 10:21:43,232] Trial 35 finished with value: 0.871592369171208 and parameters: {'n_estimators': 370, 'max_depth': 9, 'learning_rate': 0.0706357630618904, 'gamma': 0.11828426674429565}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:21:53,378] Trial 36 finished with value: 0.8709767866086414 and parameters: {'n_estimators': 366, 'max_depth': 9, 'learning_rate': 0.07209040273337715, 'gamma': 0.05966506670919492}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:22:00,627] Trial 37 finished with value: 0.8699336751178628 and parameters: {'n_estimators': 261, 'max_depth': 8, 'learning_rate': 0.2002267886548257, 'gamma': 0.042511157772127176}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:22:09,563] Trial 38 finished with value: 0.8598653087118773 and parameters: {'n_estimators': 348, 'max_depth': 7, 'learning_rate': 0.01734143992725548, 'gamma': 0.26445414986155613}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:22:16,763] Trial 39 finished with value: 0.8709286384860768 and parameters: {'n_estimators': 307, 'max_depth': 8, 'learning_rate': 0.11489249303676938, 'gamma': 0.6087814598301459}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:22:26,517] Trial 40 finished with value: 0.8714174947646092 and parameters: {'n_estimators': 375, 'max_depth': 9, 'learning_rate': 0.06939628860117261, 'gamma': 0.22249391929995943}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:22:36,379] Trial 41 finished with value: 0.8712034695677447 and parameters: {'n_estimators': 378, 'max_depth': 9, 'learning_rate': 0.07239214984017825, 'gamma': 0.24040363366384604}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:22:46,029] Trial 42 finished with value: 0.8698305665214141 and parameters: {'n_estimators': 378, 'max_depth': 8, 'learning_rate': 0.03771114979232438, 'gamma': 0.325536938042157}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:22:51,928] Trial 43 finished with value: 0.8681628442460532 and parameters: {'n_estimators': 339, 'max_depth': 9, 'learning_rate': 0.12584847929519993, 'gamma': 4.627138844859839}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:22:58,469] Trial 44 finished with value: 0.8699870192821517 and parameters: {'n_estimators': 357, 'max_depth': 7, 'learning_rate': 0.22096413639236842, 'gamma': 0.4416483717949729}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:23:07,316] Trial 45 finished with value: 0.8712167983109268 and parameters: {'n_estimators': 383, 'max_depth': 8, 'learning_rate': 0.0778211696390867, 'gamma': 0.19669796156181488}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:23:18,634] Trial 46 finished with value: 0.8712251333247684 and parameters: {'n_estimators': 384, 'max_depth': 10, 'learning_rate': 0.07720928622871001, 'gamma': 0.17384872302862653}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:23:24,608] Trial 47 finished with value: 0.8414661375184236 and parameters: {'n_estimators': 387, 'max_depth': 2, 'learning_rate': 0.04265147382667393, 'gamma': 4.220730801027088}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:23:37,869] Trial 48 finished with value: 0.8695176734445316 and parameters: {'n_estimators': 377, 'max_depth': 10, 'learning_rate': 0.023656352097775755, 'gamma': 0.6307475781227365}. Best is trial 35 with value: 0.871592369171208.\n",
      "[I 2025-11-16 10:23:44,491] Trial 49 finished with value: 0.8695862410559961 and parameters: {'n_estimators': 174, 'max_depth': 10, 'learning_rate': 0.07882795408126209, 'gamma': 3.6046415496284596}. Best is trial 35 with value: 0.871592369171208.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB best F1: 0.871592369171208\n",
      "XGB best params: {'n_estimators': 370, 'max_depth': 9, 'learning_rate': 0.0706357630618904, 'gamma': 0.11828426674429565}\n"
     ]
    }
   ],
   "source": [
    "def objective_xgb(trial: optuna.Trial) -> float:\n",
    "    model_params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 400),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"eval_metric\": \"logloss\",\n",
    "    }\n",
    "\n",
    "    fold_scores = perform_CV(\n",
    "        data=kick_transformed,\n",
    "        model_class=XGBClassifier,\n",
    "        model_params=model_params,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "    return mean_f1\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_xgb = optuna.create_study(direction=\"maximize\", study_name=\"xgb_f1\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "\n",
    "print(\"XGB best F1:\", study_xgb.best_value)\n",
    "print(\"XGB best params:\", study_xgb.best_params)\n",
    "\n",
    "df_xgb_cat = study_xgb.trials_dataframe()\n",
    "df_xgb_cat.to_csv(\"../results/optuna_xgb_trials.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b987c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 15:55:15,462] A new study created in memory with name: lr_f1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79a30f636d7425ebefd81edfc9f72cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 15:55:49,565] Trial 0 finished with value: 0.8039321655012264 and parameters: {'penalty': 'l2', 'C': 3.203430277457991}. Best is trial 0 with value: 0.8039321655012264.\n",
      "[I 2025-11-15 15:56:38,493] Trial 1 finished with value: 0.804152703664686 and parameters: {'penalty': 'l1', 'C': 0.14138073711070656}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:57:23,174] Trial 2 finished with value: 0.8040577586809678 and parameters: {'penalty': 'l1', 'C': 7.65503165689389}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:57:30,494] Trial 3 finished with value: 0.7954142533791833 and parameters: {'penalty': 'l2', 'C': 0.0019168861539635826}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:57:59,011] Trial 4 finished with value: 0.8031096344239298 and parameters: {'penalty': 'l2', 'C': 1.7295219197797516}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:58:13,234] Trial 5 finished with value: 0.7978365493619296 and parameters: {'penalty': 'l2', 'C': 0.1428805405699933}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:59:39,597] Trial 6 finished with value: 0.8040244595001959 and parameters: {'penalty': 'l1', 'C': 5.649929445115033}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 15:59:45,184] Trial 7 finished with value: 0.7910195281557055 and parameters: {'penalty': 'l1', 'C': 0.001515800637001579}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:00:27,904] Trial 8 finished with value: 0.8040378106912449 and parameters: {'penalty': 'l1', 'C': 9.328396589105388}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:00:55,893] Trial 9 finished with value: 0.8028580627117637 and parameters: {'penalty': 'l2', 'C': 1.3939687134583878}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:01:17,269] Trial 10 finished with value: 0.8027605227497332 and parameters: {'penalty': 'l1', 'C': 0.041152987666928625}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:01:27,261] Trial 11 finished with value: 0.8041030038985572 and parameters: {'penalty': 'l1', 'C': 86.92373267113412}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:01:41,641] Trial 12 finished with value: 0.8000652619819986 and parameters: {'penalty': 'l1', 'C': 0.02431823842419314}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:01:51,881] Trial 13 finished with value: 0.8040830178006573 and parameters: {'penalty': 'l1', 'C': 89.538752517193}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:02:01,715] Trial 14 finished with value: 0.8040896582055804 and parameters: {'penalty': 'l1', 'C': 89.90957472109224}. Best is trial 1 with value: 0.804152703664686.\n",
      "[I 2025-11-15 16:03:34,508] Trial 15 finished with value: 0.8041638209883251 and parameters: {'penalty': 'l1', 'C': 0.3449980898859952}. Best is trial 15 with value: 0.8041638209883251.\n",
      "[I 2025-11-15 16:04:28,725] Trial 16 finished with value: 0.8041157557697837 and parameters: {'penalty': 'l1', 'C': 0.17440826307847038}. Best is trial 15 with value: 0.8041638209883251.\n",
      "[I 2025-11-15 16:04:40,432] Trial 17 finished with value: 0.7959220654469126 and parameters: {'penalty': 'l1', 'C': 0.013010350624506652}. Best is trial 15 with value: 0.8041638209883251.\n",
      "[I 2025-11-15 16:06:25,667] Trial 18 finished with value: 0.8041952997933223 and parameters: {'penalty': 'l1', 'C': 0.4627768924709329}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:08:22,784] Trial 19 finished with value: 0.8041710864324123 and parameters: {'penalty': 'l1', 'C': 0.5744440320924616}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:10:00,925] Trial 20 finished with value: 0.8041674346400278 and parameters: {'penalty': 'l1', 'C': 0.522533314831677}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:11:52,492] Trial 21 finished with value: 0.8041059213720759 and parameters: {'penalty': 'l1', 'C': 0.9706883132618757}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:13:45,733] Trial 22 finished with value: 0.804157260570894 and parameters: {'penalty': 'l1', 'C': 0.7830926328587215}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:15:15,988] Trial 23 finished with value: 0.804125367104621 and parameters: {'penalty': 'l1', 'C': 0.3257710014559326}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:15:44,542] Trial 24 finished with value: 0.8036584029631448 and parameters: {'penalty': 'l1', 'C': 0.0687608977102625}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:16:11,976] Trial 25 finished with value: 0.8040813094649982 and parameters: {'penalty': 'l1', 'C': 25.631324626354456}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:16:20,957] Trial 26 finished with value: 0.7951864279217113 and parameters: {'penalty': 'l2', 'C': 0.008754092037203742}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:17:47,086] Trial 27 finished with value: 0.8041556676021411 and parameters: {'penalty': 'l1', 'C': 0.507947777888864}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:19:51,040] Trial 28 finished with value: 0.80407051830296 and parameters: {'penalty': 'l1', 'C': 2.325741780695952}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:20:26,419] Trial 29 finished with value: 0.8040602007455215 and parameters: {'penalty': 'l2', 'C': 3.743635670885835}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:20:54,557] Trial 30 finished with value: 0.8038113502227603 and parameters: {'penalty': 'l1', 'C': 0.07587891757099914}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:22:54,842] Trial 31 finished with value: 0.8041936162348756 and parameters: {'penalty': 'l1', 'C': 0.4540682154201854}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:24:43,913] Trial 32 finished with value: 0.8041631413075221 and parameters: {'penalty': 'l1', 'C': 0.6377584833956377}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:25:52,431] Trial 33 finished with value: 0.804098756294135 and parameters: {'penalty': 'l1', 'C': 0.20462921853406002}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:26:26,550] Trial 34 finished with value: 0.8038884039427586 and parameters: {'penalty': 'l1', 'C': 0.08053972522046116}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:27:57,585] Trial 35 finished with value: 0.8041873608839378 and parameters: {'penalty': 'l1', 'C': 0.359732654485548}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:28:25,367] Trial 36 finished with value: 0.8029130714223202 and parameters: {'penalty': 'l2', 'C': 1.4579110489968214}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:29:36,136] Trial 37 finished with value: 0.8041192708751043 and parameters: {'penalty': 'l1', 'C': 0.21692088782308183}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:30:16,873] Trial 38 finished with value: 0.8040611604387806 and parameters: {'penalty': 'l1', 'C': 11.014955241269627}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:30:51,064] Trial 39 finished with value: 0.8039895119340817 and parameters: {'penalty': 'l2', 'C': 3.429537117656205}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:31:29,374] Trial 40 finished with value: 0.804048944665362 and parameters: {'penalty': 'l1', 'C': 0.10689027185085612}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:33:10,106] Trial 41 finished with value: 0.8041868371518321 and parameters: {'penalty': 'l1', 'C': 0.4502861094103402}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:34:54,043] Trial 42 finished with value: 0.8040978021241123 and parameters: {'penalty': 'l1', 'C': 1.0734874453585648}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:46:03,309] Trial 43 finished with value: 0.8041509175959781 and parameters: {'penalty': 'l1', 'C': 0.37949157320283755}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:48:29,926] Trial 44 finished with value: 0.8041234728917095 and parameters: {'penalty': 'l1', 'C': 1.939194494663784}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:48:47,505] Trial 45 finished with value: 0.8026115518482897 and parameters: {'penalty': 'l1', 'C': 0.03908340790375344}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:49:43,476] Trial 46 finished with value: 0.8041302954657402 and parameters: {'penalty': 'l1', 'C': 0.19125137981113388}. Best is trial 18 with value: 0.8041952997933223.\n",
      "[I 2025-11-15 16:50:27,415] Trial 47 finished with value: 0.8042505633259674 and parameters: {'penalty': 'l2', 'C': 5.809841990405055}. Best is trial 47 with value: 0.8042505633259674.\n",
      "[I 2025-11-15 16:51:21,354] Trial 48 finished with value: 0.8041708855043395 and parameters: {'penalty': 'l2', 'C': 35.99637092292645}. Best is trial 47 with value: 0.8042505633259674.\n",
      "[I 2025-11-15 16:52:04,785] Trial 49 finished with value: 0.8042921221317878 and parameters: {'penalty': 'l2', 'C': 5.950374724789477}. Best is trial 49 with value: 0.8042921221317878.\n",
      "LR best F1: 0.8042921221317878\n",
      "LR best params: {'penalty': 'l2', 'C': 5.950374724789477}\n"
     ]
    }
   ],
   "source": [
    "def objective_lr(trial: optuna.Trial) -> float:\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "    solver = \"liblinear\"  # supports both l1 and l2\n",
    "\n",
    "    model_params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-3, 100.0, log=True),\n",
    "        \"penalty\": penalty,\n",
    "        \"solver\": solver,\n",
    "        \"max_iter\": 1000,\n",
    "    }\n",
    "\n",
    "    fold_scores = perform_CV(\n",
    "        data=kick_transformed,\n",
    "        model_class=LogisticRegression,\n",
    "        model_params=model_params,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "    return mean_f1\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_lr = optuna.create_study(direction=\"maximize\", study_name=\"lr_f1\")\n",
    "study_lr.optimize(objective_lr, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"LR best F1:\", study_lr.best_value)\n",
    "print(\"LR best params:\", study_lr.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71400a",
   "metadata": {},
   "source": [
    "Optimization (XGBoost) Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67c87fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for category: Theater\n",
      "XGB best F1 for Theater: 0.8092362504895695\n",
      "XGB best params for Theater: {'n_estimators': 293, 'max_depth': 6, 'learning_rate': 0.0034021548922418725, 'gamma': 2.0946971222798467}\n",
      "Optimizing for category: Technology\n",
      "XGB best F1 for Technology: 0.8730187080472896\n",
      "XGB best params for Technology: {'n_estimators': 348, 'max_depth': 9, 'learning_rate': 0.01877675526370727, 'gamma': 2.671432995667378}\n",
      "Optimizing for category: Dance\n",
      "XGB best F1 for Dance: 0.8038695078740385\n",
      "XGB best params for Dance: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.13188114596703313, 'gamma': 4.1070555608529915}\n",
      "Optimizing for category: Film & Video\n",
      "XGB best F1 for Film & Video: 0.8979781407635491\n",
      "XGB best params for Film & Video: {'n_estimators': 355, 'max_depth': 3, 'learning_rate': 0.23874158155646538, 'gamma': 0.9203490652325381}\n",
      "Optimizing for category: Music\n",
      "XGB best F1 for Music: 0.8963609845152269\n",
      "XGB best params for Music: {'n_estimators': 356, 'max_depth': 5, 'learning_rate': 0.03083965183551209, 'gamma': 4.242163339581606}\n",
      "Optimizing for category: Comics\n",
      "XGB best F1 for Comics: 0.9237693146302792\n",
      "XGB best params for Comics: {'n_estimators': 369, 'max_depth': 9, 'learning_rate': 0.039492757347250114, 'gamma': 1.4969443502327442}\n",
      "Optimizing for category: Publishing\n",
      "XGB best F1 for Publishing: 0.8647584706728771\n",
      "XGB best params for Publishing: {'n_estimators': 121, 'max_depth': 8, 'learning_rate': 0.05027572392306775, 'gamma': 4.589500712901272}\n",
      "Optimizing for category: Food\n",
      "XGB best F1 for Food: 0.7786354487150678\n",
      "XGB best params for Food: {'n_estimators': 222, 'max_depth': 4, 'learning_rate': 0.21568518924262597, 'gamma': 1.014468541738383}\n",
      "Optimizing for category: None\n",
      "XGB best F1 for None: 0.9832682949763745\n",
      "XGB best params for None: {'n_estimators': 347, 'max_depth': 6, 'learning_rate': 0.12887139900666725, 'gamma': 2.5652958113058646}\n",
      "Optimizing for category: Journalism\n",
      "XGB best F1 for Journalism: 0.6165156631278472\n",
      "XGB best params for Journalism: {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.08714708204977763, 'gamma': 0.8899954360013724}\n",
      "Optimizing for category: Games\n",
      "XGB best F1 for Games: 0.9127095188729382\n",
      "XGB best params for Games: {'n_estimators': 102, 'max_depth': 4, 'learning_rate': 0.08900502947497695, 'gamma': 0.9551178960876545}\n",
      "Optimizing for category: Fashion\n",
      "XGB best F1 for Fashion: 0.8362768786047482\n",
      "XGB best params for Fashion: {'n_estimators': 389, 'max_depth': 2, 'learning_rate': 0.2573850147732169, 'gamma': 0.4078747767273293}\n",
      "Optimizing for category: Art\n",
      "XGB best F1 for Art: 0.8814635850746804\n",
      "XGB best params for Art: {'n_estimators': 347, 'max_depth': 7, 'learning_rate': 0.11132637540148047, 'gamma': 0.6917612555184253}\n",
      "Optimizing for category: Design\n",
      "XGB best F1 for Design: 0.8573381754835537\n",
      "XGB best params for Design: {'n_estimators': 375, 'max_depth': 3, 'learning_rate': 0.15035672496825442, 'gamma': 1.3173785478574964}\n",
      "Optimizing for category: Photography\n",
      "XGB best F1 for Photography: 0.801516623777669\n",
      "XGB best params for Photography: {'n_estimators': 329, 'max_depth': 3, 'learning_rate': 0.04629566181566946, 'gamma': 0.5307260253721594}\n",
      "Optimizing for category: Crafts\n",
      "XGB best F1 for Crafts: 0.6453443327766906\n",
      "XGB best params for Crafts: {'n_estimators': 225, 'max_depth': 4, 'learning_rate': 0.2160719764246308, 'gamma': 0.30308397721791874}\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "\n",
    "best_xgb_cat_results = []\n",
    "for cat_parent_name in kick_transformed['cat_parent_name'].unique():\n",
    "    print(f\"Optimizing for category: {cat_parent_name}\")\n",
    "\n",
    "    def objective_cat(trial: optuna.Trial) -> float:\n",
    "        model_params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 400),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"eval_metric\": \"logloss\",\n",
    "        }\n",
    "\n",
    "        fold_scores = perform_CV(\n",
    "            data=kick_transformed[kick_transformed['cat_parent_name'] == cat_parent_name],\n",
    "            model_class=XGBClassifier,\n",
    "            model_params=model_params,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "        return mean_f1\n",
    "\n",
    "    study_cat = optuna.create_study(direction=\"maximize\", study_name=f\"xgb_f1_{cat_parent_name}\")\n",
    "    study_cat.optimize(objective_cat, n_trials=100)\n",
    "\n",
    "    print(f\"XGB best F1 for {cat_parent_name}:\", study_cat.best_value)\n",
    "    print(f\"XGB best params for {cat_parent_name}:\", study_cat.best_params)\n",
    "    best_xgb_cat_results.append({\n",
    "        \"cat_parent_name\": cat_parent_name,\n",
    "        \"best_f1\": study_cat.best_value,\n",
    "        \"best_params\": study_cat.best_params\n",
    "    })\n",
    "    \n",
    "    # XGBoost study\n",
    "    df_xgb_cat = study_cat.trials_dataframe()\n",
    "    df_xgb_cat.to_csv(f\"../results/model_results/optuna_{cat_parent_name}_xgb_trials.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a17bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating best XGB model for category: Theater\n",
      "Mean F1 for Theater with best XGB params: 0.7886\n",
      "Evaluating best XGB model for category: Technology\n",
      "Mean F1 for Technology with best XGB params: 0.8672\n",
      "Evaluating best XGB model for category: Dance\n",
      "Mean F1 for Dance with best XGB params: 0.7775\n",
      "Evaluating best XGB model for category: Film & Video\n",
      "Mean F1 for Film & Video with best XGB params: 0.8935\n",
      "Evaluating best XGB model for category: Music\n",
      "Mean F1 for Music with best XGB params: 0.8928\n",
      "Evaluating best XGB model for category: Comics\n",
      "Mean F1 for Comics with best XGB params: 0.9228\n",
      "Evaluating best XGB model for category: Publishing\n",
      "Mean F1 for Publishing with best XGB params: 0.8610\n",
      "Evaluating best XGB model for category: Food\n",
      "Mean F1 for Food with best XGB params: 0.7682\n",
      "Evaluating best XGB model for category: None\n",
      "Mean F1 for None with best XGB params: 0.9828\n",
      "Evaluating best XGB model for category: Journalism\n",
      "Mean F1 for Journalism with best XGB params: 0.6016\n",
      "Evaluating best XGB model for category: Games\n",
      "Mean F1 for Games with best XGB params: 0.9091\n",
      "Evaluating best XGB model for category: Fashion\n",
      "Mean F1 for Fashion with best XGB params: 0.8263\n",
      "Evaluating best XGB model for category: Art\n",
      "Mean F1 for Art with best XGB params: 0.8772\n",
      "Evaluating best XGB model for category: Design\n",
      "Mean F1 for Design with best XGB params: 0.8398\n",
      "Evaluating best XGB model for category: Photography\n",
      "Mean F1 for Photography with best XGB params: 0.7908\n",
      "Evaluating best XGB model for category: Crafts\n",
      "Mean F1 for Crafts with best XGB params: 0.6304\n"
     ]
    }
   ],
   "source": [
    "xgb_results_df = pd.read_csv(\"../results/optuna_xgb_trials.csv\")\n",
    "\n",
    "# Extract parameters of the best trial\n",
    "best_trial = xgb_results_df.loc[xgb_results_df['value'].idxmax()]\n",
    "xgb_best_params = {\n",
    "    'n_estimators': int(best_trial['params_n_estimators']),\n",
    "    'max_depth': int(best_trial['params_max_depth']),\n",
    "    'learning_rate': float(best_trial['params_learning_rate']),\n",
    "    'gamma': float(best_trial['params_gamma']),\n",
    "    'eval_metric': 'logloss',\n",
    "}\n",
    "xgb_model = XGBClassifier(**xgb_best_params)\n",
    "\n",
    "# Use perform_CV on categorical subsets using best params\n",
    "for cat_parent_name in kick_transformed['cat_parent_name'].unique():\n",
    "    print(f\"Evaluating best XGB model for category: {cat_parent_name}\")\n",
    "    fold_scores = perform_CV(\n",
    "        data=kick_transformed[kick_transformed['cat_parent_name'] == cat_parent_name],\n",
    "        model_class=XGBClassifier,\n",
    "        model_params=xgb_best_params,\n",
    "        verbose=False,\n",
    "    )\n",
    "    mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "    print(f\"Mean F1 for {cat_parent_name} with best XGB params: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7af6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "\taccuracy=0.8408, \n",
      "\tprecision=0.8736, \n",
      "\trecall=0.8692, \n",
      "\tf1=0.8714\n",
      "\n",
      "Fold 2\n",
      "\taccuracy=0.8417, \n",
      "\tprecision=0.8721, \n",
      "\trecall=0.8730, \n",
      "\tf1=0.8725\n",
      "\n",
      "Fold 3\n",
      "\taccuracy=0.8395, \n",
      "\tprecision=0.8691, \n",
      "\trecall=0.8728, \n",
      "\tf1=0.8710\n",
      "\n",
      "Fold 4\n",
      "\taccuracy=0.8372, \n",
      "\tprecision=0.8669, \n",
      "\trecall=0.8716, \n",
      "\tf1=0.8692\n",
      "\n",
      "Fold 5\n",
      "\taccuracy=0.8379, \n",
      "\tprecision=0.8700, \n",
      "\trecall=0.8685, \n",
      "\tf1=0.8693\n",
      "Average F1 across all data with ensemble XGB: 0.8707\n"
     ]
    }
   ],
   "source": [
    "def perform_CV_xgb_ensemble(\n",
    "        data: pd.DataFrame,\n",
    "        verbose = False\n",
    "    ) -> list[dict[str, float]]:\n",
    "    df = data.copy()\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    numerical_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    train = df.drop(columns=[\"is_successful\"])\n",
    "    target = df[\"is_successful\"]\n",
    "\n",
    "    train[numerical_cols] = train[numerical_cols].astype(\"float64\")\n",
    "\n",
    "    # ====================================\n",
    "    #        CROSS-VALIDATION LOOP\n",
    "    # ====================================\n",
    "    fold_scores = []\n",
    "    skf = StratifiedKFold(n_splits=K_splits, shuffle=True, random_state=random_state)\n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(train, target)):\n",
    "        if verbose:\n",
    "            print(f\"\\nFold {i+1}\")\n",
    "\n",
    "        X_train, X_test = train.iloc[train_idx], train.iloc[test_idx]\n",
    "        y_train, y_test = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "        # container for predictions on the full test fold\n",
    "        y_pred_full = pd.Series(index=X_test.index, dtype=int)\n",
    "        \n",
    "        # ====================================\n",
    "        #           TRAIN AND EVALUTATE (ONE PER CATEGORY PARENT)\n",
    "        # ====================================\n",
    "        for cat_parent_name in data['cat_parent_name'].unique():\n",
    "            # get data for this category\n",
    "            train_mask = (X_train['cat_parent_name'] == cat_parent_name)\n",
    "            test_mask = (X_test['cat_parent_name'] == cat_parent_name)\n",
    "            X_train_cat = X_train[train_mask]\n",
    "            y_train_cat = y_train[train_mask]\n",
    "            X_test_cat = X_test[test_mask]\n",
    "            # y_test_cat = y_test[test_mask]\n",
    "\n",
    "            # get best params:\n",
    "            best_params_df = pd.read_csv(f\"../results/model_results/xgb_categorical/optuna_{cat_parent_name}_xgb_trials.csv\")\n",
    "            best_trial = best_params_df.loc[best_params_df['value'].idxmax()]\n",
    "            best_params = {\n",
    "                'n_estimators': int(best_trial['params_n_estimators']),\n",
    "                'max_depth': int(best_trial['params_max_depth']),\n",
    "                'learning_rate': float(best_trial['params_learning_rate']),\n",
    "                'gamma': float(best_trial['params_gamma']),\n",
    "                'eval_metric': 'logloss',\n",
    "            }\n",
    "\n",
    "            # preprocess per category (fit on cat-train, apply to cat-test)\n",
    "            X_train_proc, X_test_proc = preprocess_data(\n",
    "                X_train_cat,\n",
    "                X_test_cat,\n",
    "                categorical_cols=categorical_cols,\n",
    "                numerical_cols=numerical_cols,\n",
    "            )\n",
    "\n",
    "            model = XGBClassifier(**best_params)\n",
    "            model.fit(X_train_proc, y_train_cat)\n",
    "\n",
    "            y_pred_cat = model.predict(X_test_proc)\n",
    "\n",
    "            # write predictions back into the global y_pred_full\n",
    "            y_pred_full.loc[test_mask] = y_pred_cat\n",
    "\n",
    "        # Now evaluate ensemble on this fold\n",
    "        y_true_fold = y_test\n",
    "        y_pred_fold = y_pred_full\n",
    "\n",
    "        fold_result = {\n",
    "            \"accuracy\": accuracy_score(y_true_fold, y_pred_fold),\n",
    "            \"precision\": precision_score(y_true_fold, y_pred_fold, zero_division=0),\n",
    "            \"recall\": recall_score(y_true_fold, y_pred_fold, zero_division=0),\n",
    "            \"f1\": f1_score(y_true_fold, y_pred_fold, zero_division=0),\n",
    "        }\n",
    "        fold_scores.append(fold_result)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\taccuracy={fold_result['accuracy']:.4f}, \\n\"\n",
    "                f\"\\tprecision={fold_result['precision']:.4f}, \\n\"\n",
    "                f\"\\trecall={fold_result['recall']:.4f}, \\n\"\n",
    "                f\"\\tf1={fold_result['f1']:.4f}\"\n",
    "            )\n",
    "\n",
    "    return fold_scores\n",
    "\n",
    "fold_scores = perform_CV_xgb_ensemble(kick_transformed, verbose=True)\n",
    "# get average f1 score across folds\n",
    "mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "print(f\"Average F1 across all data with ensemble XGB: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97df5a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\taccuracy=0.8403, \n",
      "\tprecision=0.8718, \n",
      "\trecall=0.8708, \n",
      "\tf1=0.8713\n",
      "Fold 2\n",
      "\taccuracy=0.8417, \n",
      "\tprecision=0.8679, \n",
      "\trecall=0.8787, \n",
      "\tf1=0.8733\n",
      "Fold 3\n",
      "\taccuracy=0.8392, \n",
      "\tprecision=0.8676, \n",
      "\trecall=0.8744, \n",
      "\tf1=0.8710\n",
      "Fold 4\n",
      "\taccuracy=0.8373, \n",
      "\tprecision=0.8661, \n",
      "\trecall=0.8728, \n",
      "\tf1=0.8694\n",
      "Fold 5\n",
      "\taccuracy=0.8380, \n",
      "\tprecision=0.8681, \n",
      "\trecall=0.8715, \n",
      "\tf1=0.8698\n",
      "Average F1 across all data with best XGB params: 0.8710\n"
     ]
    }
   ],
   "source": [
    "fold_scores = perform_CV(kick_transformed, XGBClassifier, xgb_best_params, verbose=True)\n",
    "# get average f1 score across folds\n",
    "\n",
    "mean_f1 = float(np.mean([fold[\"f1\"] for fold in fold_scores]))\n",
    "print(f\"Average F1 across all data with best XGB params: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94eb82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMSE802_PROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
